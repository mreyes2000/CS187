{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ae679ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs187-2021/project2.git .tmp\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56665f9d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "013a1303",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9a046",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afda95",
   "metadata": {
    "colab_type": "text",
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# CS187\n",
    "## Project 2: Sequence labeling â€“ The slot filling task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7dc1a",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "The second segment of the project involves a sequence labeling task, in which the goal is to label the tokens in a text. Many NLP tasks have this general form. Most famously is the task of _part-of-speech labeling_ as you explored in lab 2-4, where the tokens in a text are to be labeled with their part of speech (noun, verb, preposition, etc.). In this project segment, however, you'll use sequence labeling to implement a system for filling the slots in a template that is intended to describe the meaning of an ATIS query. For instance, the sentence \n",
    "\n",
    "    What's the earliest arriving flight between Boston and Washington DC?\n",
    "    \n",
    "might be associated with the following slot-filled template: \n",
    "\n",
    "    flight_id\n",
    "        fromloc.cityname: boston\n",
    "        toloc.cityname: washington\n",
    "        toloc.state: dc\n",
    "        flight_mod: earliest arriving\n",
    "    \n",
    "You may wonder how this task is a sequence labeling task. We label each word in the source sentence with a tag taken from a set of tags that correspond to the slot-labels. For each slot-label, say `flight_mod`, there are two tags: `B-flight_mod` and `I-flight_mod`. These are used to mark the beginning (B) or interior (I) of a phrase that fills the given slot. In addition, there is a tag for other (O) words that are not used to fill any slot. (This technique is thus known as IOB encoding.) Thus the sample sentence would be labeled as follows:\n",
    "\n",
    "| Token   | Label    |\n",
    "| :------ | :----- | \n",
    "| `BOS` | `O` |\n",
    "| `what's` | `O` |\n",
    "| `the` | `O` |\n",
    "| `earliest` | `B-flight_mod` |\n",
    "| `arriving` | `I-flight_mod` |\n",
    "| `flight` | `O` |\n",
    "| `between` | `O` |\n",
    "| `boston` | `B-fromloc.city_name` |\n",
    "| `and` | `O` |\n",
    "| `washington` | `B-toloc.city_name` |\n",
    "| `dc` | `B-toloc.state_code` |\n",
    "| `EOS` | `O` |\n",
    "\n",
    "> See below for information about the `BOS` and `EOS` tokens. \n",
    "\n",
    "The template itself is associated with the question type for the sentence, perhaps as recovered from the sentence in the last project segment.\n",
    "\n",
    "In this segment, you'll implement three methods for sequence labeling: a hidden Markov model (HMM) and two recurrent neural networks, a simple RNN and a long short-term memory network (LSTM). By the end of this homework, you should have grasped the pros and cons of the statistical and neural approaches.\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. Implement an HMM-based approach to sequence labeling.\n",
    "2. Implement an RNN-based approach to sequence labeling.\n",
    "3. Implement an LSTM-based approach to sequence labeling.\n",
    "4. (Optional) Compare the performances of HMM and RNN/LSTM with different amounts of training data. Discuss the pros and cons of the HMM approach and the neural approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207529b0",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3fc01dc",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import wget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.legacy as tt\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95a4517",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# GPU check, sets runtime type to \"GPU\" where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca704a",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Loading data\n",
    "\n",
    "We download the ATIS dataset, already presplit into training, validation (dev), and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e78a18",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# Prepare to download needed data\n",
    "def download_if_needed(filename, source='./', dest='./'):\n",
    "    os.makedirs(data_path, exist_ok=True) # ensure destination\n",
    "    os.path.exists(f\"./{dest}{filename}\") or wget.download(source + filename, out=dest)\n",
    "\n",
    "source_path = \"https://raw.githubusercontent.com/nlp-course/data/master/ATIS/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "# Download files\n",
    "for filename in [\"atis.train.txt\",\n",
    "                 \"atis.dev.txt\",\n",
    "                 \"atis.test.txt\"\n",
    "                ]:\n",
    "    download_if_needed(filename, source_path, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd0a7ff",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We again use `torchtext` to load data and convert words to indices in the vocabulary. We use one field `TEXT` for processing the question, and another field `TAG` for processing the sequence labels.\n",
    "\n",
    "We treat words occurring fewer than three times in the training data as _unknown words_. They'll be replaced by the unknown word type `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2ca342",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "MIN_FREQ = 3\n",
    "\n",
    "TEXT = tt.data.Field(init_token=\"<bos>\", batch_first=False) # batches are of size max_len x bsz\n",
    "TAG = tt.data.Field(init_token=\"<bos>\", batch_first=False)  # ditto\n",
    "fields = (('text', TEXT), ('tag', TAG))\n",
    "\n",
    "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n",
    "  fields=fields, \n",
    "  path='./data/', \n",
    "  train='atis.train.txt',\n",
    "  validation='atis.dev.txt',\n",
    "  test='atis.test.txt'\n",
    ")\n",
    "\n",
    "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "TAG.build_vocab(train.tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c10db",
   "metadata": {},
   "source": [
    "We can get some sense of the datasets by looking at the size and some elements of the text and tag vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debede0c",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of English vocabulary: 518\n",
      "Most common English words: [('BOS', 4274), ('EOS', 4274), ('to', 3682), ('from', 3203), ('flights', 2075), ('the', 1745), ('on', 1343), ('flight', 1035), ('me', 1005), ('what', 985)]\n",
      "\n",
      "Number of tags: 104\n",
      "Most common tags: [('O', 38967), ('B-toloc.city_name', 3751), ('B-fromloc.city_name', 3726), ('I-toloc.city_name', 1039), ('B-depart_date.day_name', 835), ('I-fromloc.city_name', 636), ('B-airline_name', 610), ('B-depart_time.period_of_day', 555), ('I-airline_name', 374), ('B-depart_date.day_number', 351)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Size of English vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Most common English words: {TEXT.vocab.freqs.most_common(10)}\\n\")\n",
    "\n",
    "print(f\"Number of tags: {len(TAG.vocab)}\")\n",
    "print(f\"Most common tags: {TAG.vocab.freqs.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3d993",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Special tokens and tags\n",
    "\n",
    "You'll have already noticed the `BOS` and `EOS`, special tokens that the dataset developers used to indicate the beginning and end of the sentence; we'll leave them in the data.\n",
    "\n",
    "We've also passed in `init_token=\"<bos>\"` for both torchtext fields. Torchtext will prepend these to the sequence of words and tags. This relieves us from estimating the initial distribution of tags and tokens in HMMs, since we always start with a token `<bos>` whose tag is also `<bos>`. We'll be able to refer to these tags as exemplified here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f36c9713",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial tag string: <bos>\n",
      "Initial tag id:     2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Initial tag string: {TAG.init_token}\n",
    "Initial tag id:     {TAG.vocab.stoi[TAG.init_token]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d6e02",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Finally, since `torchtext` will be providing the sentences in the training corpus in \"batches\", `torchtext` will force the sentences within a batch to be the same length by padding them with a special token. Again, we can access that token as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a839c984",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pad tag string: <pad>\n",
      "Pad tag id:     1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Pad tag string: {TAG.pad_token}\n",
    "Pad tag id:     {TAG.vocab.stoi[TAG.pad_token]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156fc63",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now, we can iterate over the dataset using `torchtext`'s iterator. We'll use a non-trivial batch size to gain the benefit of training on multiple sentences at a shot. You'll need to be careful about the shapes of the various tensors that are being manipulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46841c0b",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 20\n",
    "\n",
    "train_iter, val_iter, test_iter = tt.data.BucketIterator.splits(\n",
    "    (train, val, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    repeat=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a972919c",
   "metadata": {},
   "source": [
    "Each batch will be a tensor of size `max_length x batch_size`. Let's examine a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d02a12",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of batch text tensor: torch.Size([22, 20])\n",
      "\n",
      "First sentence in batch\n",
      "tensor([ 2,  3, 21, 45, 88, 44,  7, 39, 28, 20, 54, 18, 22,  4,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1])\n",
      "<bos> BOS i need information for flights leaving baltimore and arriving in atlanta EOS <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "\n",
      "First tags in batch\n",
      "tensor([2, 3, 3, 3, 3, 3, 3, 3, 5, 3, 3, 3, 4, 3, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "['<bos>', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-fromloc.city_name', 'O', 'O', 'O', 'B-toloc.city_name', 'O', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "# Get the first batch\n",
    "batch = next(iter(train_iter))\n",
    "\n",
    "# What's its shape? Should be max_length x batch_size.\n",
    "print(f'Shape of batch text tensor: {batch.text.shape}\\n')\n",
    "\n",
    "# Extract the first sentence in the batch, both text and tags\n",
    "first_sentence = batch.text[:, 0]\n",
    "first_tags = batch.tag[:, 0]\n",
    "\n",
    "# Print out the first sentence, as token ids and as text\n",
    "print(\"First sentence in batch\")\n",
    "print(f\"{first_sentence}\")\n",
    "print(f\"{' '.join([TEXT.vocab.itos[i] for i in first_sentence])}\\n\")\n",
    "\n",
    "print(\"First tags in batch\")\n",
    "print(f\"{first_tags}\")\n",
    "print(f\"{[TAG.vocab.itos[i] for i in first_tags]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4decb51f",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "The goal of this project is to predict the sequence of tags `batch.tag` given a sequence of words `batch.text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f584fb",
   "metadata": {},
   "source": [
    "# Majority class labeling\n",
    "\n",
    "As usual, we can get a sense of the difficulty of the task by looking at a simple baseline, tagging every token with the majority tag. Here's a table of tag frequencies for the most frequent tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58848452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0  <unk>                           0\n",
      "  1  <pad>                           0\n",
      "  2  <bos>                         4274\n",
      "  3  O                             38967\n",
      "  4  B-toloc.city_name             3751\n",
      "  5  B-fromloc.city_name           3726\n",
      "  6  I-toloc.city_name             1039\n",
      "  7  B-depart_date.day_name        835\n",
      "  8  I-fromloc.city_name           636\n",
      "  9  B-airline_name                610\n",
      " 10  B-depart_time.period_of_day   555\n",
      " 11  I-airline_name                374\n",
      " 12  B-depart_date.day_number      351\n",
      " 13  B-depart_date.month_name      340\n",
      " 14  B-depart_time.time            321\n",
      " 15  B-round_trip                  311\n",
      " 16  I-round_trip                  303\n",
      " 17  B-depart_time.time_relative   290\n",
      " 18  B-cost_relative               281\n",
      " 19  B-flight_mod                  264\n",
      " 20  I-depart_time.time            258\n",
      " 21  B-stoploc.city_name           202\n",
      " 22  B-city_name                   191\n",
      " 23  B-arrive_time.time            182\n",
      " 24  B-class_type                  181\n",
      " 25  B-arrive_time.time_relative   162\n",
      " 26  I-class_type                  148\n",
      " 27  I-arrive_time.time            142\n",
      " 28  B-flight_stop                 141\n",
      " 29  B-airline_code                109\n",
      " 30  I-depart_date.day_number      105\n",
      " 31  I-fromloc.airport_name        103\n",
      " 32  B-toloc.state_name             84\n",
      " 33  B-toloc.state_code             81\n",
      " 34  B-arrive_date.day_name         78\n",
      " 35  B-fromloc.airport_name         75\n",
      " 36  B-depart_date.date_relative    72\n",
      " 37  B-flight_number                72\n",
      " 38  B-depart_date.today_relative   70\n",
      " 39  I-airport_name                 61\n",
      " 40  I-city_name                    53\n",
      " 41  B-arrive_time.period_of_day    51\n",
      " 42  B-fare_basis_code              51\n",
      " 43  B-flight_time                  51\n",
      " 44  B-fromloc.state_code           51\n",
      " 45  B-or                           49\n",
      " 46  B-aircraft_code                48\n",
      " 47  B-meal_description             48\n",
      " 48  B-meal                         47\n",
      " 49  I-cost_relative                45\n",
      " 50  I-stoploc.city_name            45\n",
      " 51  B-airport_name                 44\n",
      " 52  B-transport_type               43\n",
      " 53  B-fromloc.state_name           42\n",
      " 54  B-arrive_date.day_number       40\n",
      " 55  B-arrive_date.month_name       40\n",
      " 56  B-depart_time.period_mod       39\n",
      " 57  B-flight_days                  37\n",
      " 58  B-connect                      36\n",
      " 59  I-toloc.airport_name           35\n",
      " 60  B-fare_amount                  34\n",
      " 61  I-fare_amount                  33\n",
      " 62  B-economy                      32\n",
      " 63  B-toloc.airport_name           28\n",
      " 64  B-mod                          24\n",
      " 65  I-flight_time                  24\n",
      " 66  B-airport_code                 22\n",
      " 67  B-depart_date.year             20\n",
      " 68  B-toloc.airport_code           19\n",
      " 69  B-arrive_time.start_time       18\n",
      " 70  B-depart_time.end_time         18\n",
      " 71  B-depart_time.start_time       18\n",
      " 72  I-transport_type               18\n",
      " 73  B-arrive_time.end_time         17\n",
      " 74  I-arrive_time.end_time         16\n",
      " 75  B-fromloc.airport_code         14\n",
      " 76  B-restriction_code             14\n",
      " 77  I-depart_time.end_time         13\n",
      " 78  I-flight_mod                   12\n",
      " 79  I-flight_stop                  12\n",
      " 80  B-arrive_date.date_relative    10\n",
      " 81  I-toloc.state_name             10\n",
      " 82  I-restriction_code              9\n",
      " 83  B-return_date.date_relative     8\n",
      " 84  I-depart_time.start_time        8\n",
      " 85  I-economy                       8\n",
      " 86  B-state_code                    7\n",
      " 87  I-arrive_time.start_time        7\n",
      " 88  I-fromloc.state_name            7\n",
      " 89  B-state_name                    6\n",
      " 90  I-depart_date.today_relative    6\n",
      " 91  I-depart_time.period_of_day     5\n",
      " 92  B-period_of_day                 4\n",
      " 93  I-arrive_date.day_number        4\n",
      " 94  B-day_name                      3\n",
      " 95  B-meal_code                     3\n",
      " 96  B-stoploc.state_code            3\n",
      " 97  B-arrive_time.period_mod        2\n",
      " 98  B-toloc.country_name            2\n",
      " 99  I-arrive_time.time_relative     2\n",
      "100  I-meal_code                     2\n",
      "101  I-return_date.date_relative     2\n",
      "102  B-return_date.day_number        1\n",
      "103  B-return_date.month_name        1\n"
     ]
    }
   ],
   "source": [
    "def count_tags(iterator):\n",
    "  tag_counts = torch.zeros(len(TAG.vocab.itos), device=device)\n",
    "\n",
    "  for batch in iterator:\n",
    "    tags = batch.tag.view(-1)\n",
    "    tag_counts.scatter_add_(0, tags, torch.ones(tags.shape).to(device))\n",
    "\n",
    "  ## Alternative untensorized implementation for reference\n",
    "  # for batch in iterator:                # for each batch\n",
    "  #   for sent_id in range(len(batch)):   # ... each sentence in the batch\n",
    "  #     for tag in batch.tag[:, sent_id]: # ... each tag in the sentence\n",
    "  #       tag_counts[tag] += 1            # bump the tag count\n",
    "\n",
    "  # Ignore paddings\n",
    "  tag_counts[TAG.vocab.stoi[TAG.pad_token]] = 0\n",
    "  return tag_counts\n",
    "\n",
    "tag_counts = count_tags(train_iter)\n",
    "\n",
    "for tag_id in range(len(TAG.vocab.itos)):\n",
    "  print(f'{tag_id:3}  {TAG.vocab.itos[tag_id]:30}{tag_counts[tag_id].item():3.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee7847",
   "metadata": {},
   "source": [
    "It looks like the `'O'` (other) tag is, unsurprisingly, the most frequent tag (except for the padding tag). The proportion of tokens labeled with that tag (ignoring the padding tag) gives us a good baseline accuracy for this sequence labeling task. To verify that intuition, we can calculate the accuracy of the majority tag on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19a9c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.634\n"
     ]
    }
   ],
   "source": [
    "tag_counts_test = count_tags(test_iter)\n",
    "majority_baseline_accuracy = (\n",
    "  tag_counts_test[TAG.vocab.stoi['O']] \n",
    "  / tag_counts_test.sum()\n",
    ")\n",
    "print(f'Baseline accuracy: {majority_baseline_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc95234c",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# HMM for sequence labeling\n",
    "\n",
    "Having established the baseline to beat, we turn to implementing an HMM model.\n",
    "\n",
    "## Notation\n",
    "\n",
    "First, let's start with some notation. We use $\\mathcal{V} = \\langle \\mathcal{V}_1, \\mathcal{V}_2, \\ldots \\mathcal{V}_V \\rangle$ to denote the vocabulary of word types and $Q = \\langle{Q_1, Q_2, \\ldots, Q_N} \\rangle$ to denote the possible tags, which is the state space of the HMM. Thus $V$ is the number of word types in the vocabulary and $N$ is the number of states (tags).\n",
    "\n",
    "We use $\\vect{w} = w_1 \\cdots w_T \\in \\mathcal{V}^T$ to denote the string of words at \"time steps\" $t$ (where $t$ varies from $1$ to $T$). Similarly, $\\vect{q} = q_1 \\cdots q_T \\in Q^T$ denotes the corresponding sequence of states (tags)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1307444e",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Training an HMM by counting\n",
    "\n",
    "Recall that an HMM is defined via a transition matrix $A$, which stores the probability of moving from one state $Q_i$ to another $Q_j$, that is, \n",
    "\n",
    "$$A_{ij}=\\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i)$$\n",
    "\n",
    "and an emission matrix $B$, which stores the probability of generating word $\\mathcal{V}_j$ given state $Q_i$, that is, \n",
    "\n",
    "$$B_{ij}= \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)$$\n",
    "\n",
    "> As is typical in notating probabilities, we'll use abbreviations\n",
    ">\n",
    "\\begin{align}\n",
    "\\Prob(q_{t+1} \\given  q_t) &\\equiv \\Prob(q_{t+1}=Q_j  \\given  q_t=Q_i) \\\\\n",
    "\\Prob(w_t  \\given q_t) &\\equiv \\Prob(w_t=\\mathcal{V}_j  \\given q_t= Q_i)\n",
    "\\end{align}\n",
    ">\n",
    "> where the $i$ and $j$ are clear from context.\n",
    "\n",
    "In our case, since the labels are observed in the training data, we can directly use counting to determine (maximum likelihood) estimates of $A$ and $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00feef9c",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(a): Find the transition matrix\n",
    "\n",
    "The matrix $A$ contains the transition probabilities: $A_{ij}$ is the probability of moving from state $Q_i$ to state $Q_j$ in the training data, so that $\\sum^{N}_{j = 1 } A_{ij} = 1$ for all $i$. \n",
    "\n",
    "We find these probabilities by counting the number of times state $Q_j$ appears right after state $Q_i$, as a proportion of all of the transitions from $Q_i$.\n",
    "\n",
    "$$\n",
    "A_{ij} = \\frac{\\cnt{Q_i, Q_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, Q_k}+\\delta \\right)}\n",
    "$$\n",
    "\n",
    "(In the above formula, we also used add-$\\delta$ smoothing.)\n",
    "\n",
    "Using the above definition, implement the method `train_A` in the `HMM` class below, which calculates and returns the $A$ matrix as a tensor of size $N \\times N$.\n",
    "\n",
    "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal.\n",
    "\n",
    "> Remember that the training data is being delivered to you batched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a709ee",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(b): Find the emission matrix $B$\n",
    "\n",
    "Similar to the transition matrix, the emission matrix contains the emission probabilities such that $B_{ij}$ is probability of word $w_t=\\mathcal{V}_j$ conditioned on state $q_t=Q_i$.\n",
    "\n",
    "We can find this by counting as well.\n",
    "$$\n",
    "B_{ij} = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\sum_k \\left (\\cnt{Q_i, \\mathcal{V}_k} + \\delta \\right)}\n",
    "       = \\frac{\\cnt{Q_i, \\mathcal{V}_j} + \\delta}{\\cnt{Q_i} + \\delta V}\n",
    "$$\n",
    "\n",
    "Using the above definitions, implement the `train_B` method in the `HMM` class below, which calculates and returns the $B$ matrix as a tensor of size $N \\times V$.\n",
    "\n",
    "> You'll want to go ahead and implement this part now, and test it below, before moving on to the next goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47762b3",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Sequence labeling with a trained HMM\n",
    "\n",
    "Now that you're able to train an HMM by estimating the transition matrix $A$ and the emission matrix $B$, you can apply it to the task of labeling a sequence of words $\\vect{w} = w_1 \\cdots w_T$. Our goal is to find the most probable sequence of tags $\\vect{\\hat q} \\in Q^T$ given a sequence of words $\\vect{w} \\in \\mathcal{V}^T$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\vect{\\hat q} &= \\operatorname*{argmax}\\limits_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q} \\given \\vect{w})) \\\\\n",
    "& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}(\\Prob(\\vect{q},\\vect{w})) \\\\\n",
    "& = \\operatorname*{argmax}_{\\vect{q} \\in Q^T}\\left(\\Pi^{T}_{t = 1} \\Prob(w_{t} \\given q_{t})\\Prob(q_{t} \\given q_{t-1})\\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Prob(w_{t}=\\mathcal{V}_j \\given q_{t}=Q_i) = B_{ij}$, $\\Prob(q_{t}=Q_j \\given q_{t-1}=Q_{i})=A_{ij}$, and $q_0$ is the predefined initial tag `TAG.vocab.stoi[TAG.init_token]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d4f77",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Goal 1(c): Viterbi algorithm\n",
    "\n",
    "Implement the `predict` method, which should use the Viterbi algorithm to find the most likely sequence of tags for a sequence of `words`.\n",
    "\n",
    "> Warning: It may take up to 30 minutes to tag the entire test set depending on your implementation. (A fully tensorized implementation can be much faster though.) We highly recommend that you begin by experimenting with your code using a _very small subset_ of the dataset, say two or three sentences, ramping up from there.\n",
    "\n",
    "> Hint: Consider how to use vectorized computations where possible for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044c596b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We've provided you with the `evaluate` function, which takes a dataset iterator and uses `predict` on each sentence in each batch, comparing against the gold tags, to determine the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e916f7eb",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "class HMMTagger():\n",
    "    def __init__ (self, text, tag):\n",
    "        self.text = text\n",
    "        self.text_size = len(text.vocab.itos)\n",
    "\n",
    "        self.tag = tag\n",
    "        self.tag_size = len(tag.vocab.itos)\n",
    "\n",
    "        # Save pad tag and text id to ignore\n",
    "        self.pad_tag_id = tag.vocab.stoi[tag.pad_token]\n",
    "        self.pad_text_id = text.vocab.stoi[text.pad_token]\n",
    "  \n",
    "    def train_A(self, iterator, delta):\n",
    "        \"\"\"Returns A for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n",
    "        # Create A table and counts\n",
    "        A = torch.zeros(self.tag_size, self.tag_size, device=device)\n",
    "        counts = torch.zeros(self.tag_size, self.tag_size, device=device)\n",
    "\n",
    "        # Fill in the counts table\n",
    "        for batch in iterator:\n",
    "            for Q, Q_ in zip(batch.tag[:len(batch.tag) - 1], batch.tag[1:]):\n",
    "                for q, q_ in zip(Q, Q_):\n",
    "                    if q_ != self.pad_tag_id:\n",
    "                        counts[q][q_] += 1\n",
    "\n",
    "        # Add delta and divide by the sums\n",
    "        counts += delta \n",
    "        A = counts / torch.sum(counts, dim=0)\n",
    "        return A\n",
    "\n",
    "    def train_B(self, iterator, delta):\n",
    "        \"\"\"Returns B for training dataset `iterator` using add-`delta` smoothing.\"\"\"\n",
    "        # Create B table and counts\n",
    "        B = torch.zeros(self.tag_size, self.text_size, device=device)\n",
    "        counts = torch.zeros(self.tag_size, self.text_size, device=device)\n",
    "\n",
    "        # Fill in the counts table\n",
    "        for batch in iterator:\n",
    "            for Q, W in zip(batch.tag, batch.text):\n",
    "                for q, w in zip(Q, W):\n",
    "                    if q != self.pad_tag_id:\n",
    "                        counts[q][w] += 1\n",
    "    \n",
    "        # Add delta and divide by the sums\n",
    "        counts += delta\n",
    "        B = counts / torch.sum(counts, dim=0)\n",
    "        return B\n",
    "\n",
    "    def train_all(self, iterator, delta=0.01):\n",
    "        \"\"\"Stores A and B (actually, their logs) for training dataset `iterator`.\"\"\"\n",
    "        self.log_A = self.train_A(iterator, delta).log()\n",
    "        self.log_B = self.train_B(iterator, delta).log()\n",
    "    \n",
    "    def predict(self, words):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of `words`.\n",
    "        Arguments:\n",
    "          words: a tensor of size (seq_len,)\n",
    "        Returns:\n",
    "          a list of tag ids\n",
    "        \"\"\"\n",
    "        words_size = len(words)\n",
    "        \n",
    "        # Initialize dynamic programming arrays\n",
    "        v = torch.zeros(self.tag_size, words_size, device=device)\n",
    "        bp = torch.zeros(self.tag_size, words_size, device=device)\n",
    "\n",
    "        # Set initial values and take the log\n",
    "        v[2,0] = 1\n",
    "        v = v.log()\n",
    "\n",
    "        # Use viterbi's algorithm to dynamically fill in arrays\n",
    "        for i in range(1, words_size): \n",
    "            # Addition since we deal with log-probabilities\n",
    "            probs = torch.max(self.log_A + v[:,i-1].view(-1,1) + self.log_B[:,words[i]].view(1,-1), dim=0)\n",
    "            v[:,i] = probs[0]\n",
    "            bp[:,i-1] = probs[1]\n",
    "\n",
    "        # Find last label\n",
    "        tag = v[:,-1].argmax()\n",
    "        tags = [tag]\n",
    "\n",
    "        # Reconstruct labels by backtracking\n",
    "        for i in range(words_size-2,-1,-1):\n",
    "            tag = int(bp[tag,i])\n",
    "            tags.append(tag)\n",
    "\n",
    "        return tags[::-1]\n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        \"\"\"Returns the model's token accuracy on a given dataset `iterator`.\"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(iterator, leave=False):\n",
    "            for sent_id in range(len(batch)):\n",
    "                words = batch.text[:, sent_id]\n",
    "                words = words[words.ne(self.pad_text_id)] # remove paddings\n",
    "                tags_gold = batch.tag[:, sent_id]\n",
    "                tags_pred = self.predict(words)\n",
    "            for tag_gold, tag_pred in zip(tags_gold, tags_pred):\n",
    "                if tag_gold == self.pad_tag_id:  # stop once we hit padding\n",
    "                    break\n",
    "                else:\n",
    "                    total += 1\n",
    "                    if tag_pred == tag_gold:\n",
    "                        correct += 1\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e8a6de",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Putting everything together, you should now be able to train and evaluate the HMM. A correct implementation can be expected to reach above **90% test set accuracy** after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c3fe23",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.900\n",
      "Test accuracy:     0.908\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "hmm_tagger = HMMTagger(TEXT, TAG)\n",
    "hmm_tagger.train_all(train_iter)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {hmm_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {hmm_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec749584",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# RNN for Sequence Labeling\n",
    "\n",
    "HMMs work quite well for this sequence labeling task. Now let's take an alternative (and more trendy) approach: RNN/LSTM-based sequence labeling. Similar to the HMM part of this project, you will also need to train a model on the training data, and then use the trained model to decode and evaluate some testing data.\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/rnn-unfolded-figure.png\" width=600 align=right />\n",
    "\n",
    "After unfolding an RNN, the cell at time $t$ generates the observed output $\\vect{y}_t$ based on the input $\\vect{x}_t$ and the hidden state of the previous cell $\\vect{h}_{t-1}$, according to the following equations.\n",
    "\n",
    "\\begin{align*}\n",
    "\\vect{h}_t &=  \\sigma(\\vect{U} \\vect{x}_t + \\vect{V} \\vect{h}_{t-1}) \\\\\n",
    "\\vect{\\hat y}_t &= \\softmax(\\vect{W} \\vect{h}_t)\n",
    "\\end{align*}\n",
    "\n",
    "The parameters here are the elements of the matrices $\\vect{U}$, $\\vect{V}$, and $\\vect{W}$. Similar to the last project segment, we will perform the forward computation, calculate the loss, and then perform the backward computation to compute the gradients with respect to these model parameters. Finally, we will adjust the parameters opposite the direction of the gradients to minimize the loss, repeating until convergence.\n",
    "\n",
    "You've seen these kinds of neural network models before, for language modeling in lab 2-3 and sequence labeling in lab 2-5. The code there should be very helpful in implementing an `RNNTagger` class below. Consequently, we've provided very little guidance on the implementation. We do recommend you follow the steps below however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d4553f",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Goal 2(a): RNN training\n",
    "\n",
    "Implement the forward pass of the RNN tagger and the loss function. A reasonable way to proceed is to implement the following methods:\n",
    "\n",
    "1. `forward(self, text_batch)`: Performs the RNN forward computation over a whole `text_batch` (`batch.text` in the above data loading example). The `text_batch` will be of shape `max_length x batch_size`. You might run it through the following layers: an embedding layer, which maps each token index to an embedding of size `embedding_size` (so that the size of the mapped batch becomes `max_length x batch_size x embedding_size`); then an RNN, which maps each token embedding to a vector of `hidden_size` (the size of all outputs is `max_length x batch_size x hidden_size`); then a linear layer, which maps each RNN output element to a vector of size $N$ (which is commonly referred to as \"logits\", recall that $N=|Q|$, the size of the tag set).\n",
    "\n",
    "This function is expected to return `logits`, which provides a logit for each tag of each word of each sentence in the batch (structured as a tensor of size `max_length x batch_size x N`). \n",
    "\n",
    "> You might find the following functions useful: \n",
    ">\n",
    "> * [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "> * [`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "> * [`nn.RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "\n",
    "2. `compute_loss(self, logits, tags)`: Computes the loss for a batch by comparing `logits` of a batch returned by `forward` to `tags`, which stores the true tag ids for the batch. Thus `logits` is a tensor of size `max_length x batch_size x N`, and `tags` is a tensor of size `max_length x batch_size`. Note that the criterion functions in `torch` expect outputs of a certain shape, so you might need to perform some shape conversions.\n",
    "\n",
    "> You might find [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) from the last project segment useful. Note that if you use `nn.CrossEntropyLoss` then you should not use a softmax layer at the end since that's already absorbed into the loss function. Alternatively, you can use [`nn.LogSoftmax`](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) as the final sublayer in the forward pass, but then you need to use [`nn.NLLLoss`](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html), which does not contain its own softmax. We recommend the former, since working in log space is usually more numerically stable.\n",
    "\n",
    "> Be careful about the shapes/dimensions of tensors. You might find [`torch.Tensor.view`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) useful for reshaping tensors.\n",
    "\n",
    "3. `train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001)`: Trains the model on training data generated by the iterator `train_iter` and validation data `val_iter`.The `epochs` and `learning_rate` variables are the number of epochs (number of times to run through the training data) to run for and the learning rate for the optimizer, respectively. You can use the validation data to determine which model was the best one as the epocks go by. Notice that our code below assumes that during training the best model is stored so that `rnn_tagger.load_state_dict(rnn_tagger.best_model)` restores the parameters of the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d8bae",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Goal 2(b) RNN decoding\n",
    "\n",
    "Implement a method to predict the tag sequence associated with a sequence of words:\n",
    "\n",
    "1. `predict(self, text_batch)`: Returns the batched predicted tag sequences associated with a batch of sentences.\n",
    "2. `def evaluate(self, iterator)`: Returns the accuracy of the trained tagger on a dataset provided by `iterator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5dc60ef",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pj/rzsv_v_d1r9fn14fhvvjkrwc0000gn/T/ipykernel_37877/2039967788.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mRNNTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNNTagger(nn.Module): \n",
    "    def __init__ (self, text, tag, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.text = text\n",
    "        self.text_size = len(text.vocab.itos)\n",
    "        \n",
    "        self.tag = tag\n",
    "        self.tag_size = len(tag.vocab.itos)\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.text_size, self.embedding_size) # Lookup layer\n",
    "        self.rnn = nn.RNN(input_size=self.embedding_size, hidden_size=self.hidden_size, bidirectional=True)\n",
    "        self.hidden2output = nn.Linear(2 * self.hidden_size, self.tag_size)\n",
    "        \n",
    "        pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n",
    "    \n",
    "    def init_parameters(self, init_low=-0.15, init_high=0.15):\n",
    "        \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n",
    "        See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n",
    "        in-depth discussion.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(init_low, init_high)\n",
    "    \n",
    "    def forward(self, text_batch):\n",
    "        \"\"\"Performs forward, returns logits.\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, 1)\n",
    "        Returns:\n",
    "          logits: a tensor of size (seq_len, 1, self.tag_size)\n",
    "        \"\"\"\n",
    "        word_embeddings = self.word_embeddings(text_batch)\n",
    "        # h0 initialized to zeros when nothing inputted\n",
    "        output, hn = self.rnn(word_embeddings) \n",
    "        logits = self.hidden2output(output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, logits, tags):\n",
    "        return self.loss_function(logits.view(-1, self.tag_size), tags.view(-1))\n",
    "\n",
    "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "        # Switch the module to training mode\n",
    "        self.train()\n",
    "        # Use Adam to optimize the parameters\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        # Run the optimization for multiple epochs\n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter):\n",
    "                # Zero the parameter gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Input and target\n",
    "                words = batch.text # seq_len, 1\n",
    "                tags = batch.tag   # seq_len, 1\n",
    "\n",
    "                # Run forward pass and compute loss along the way.\n",
    "                logits = self.forward(words)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "\n",
    "                # Perform backpropagation\n",
    "                (loss/words.size(1)).backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optim.step()\n",
    "\n",
    "                # Training stats\n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Evaluate and track improvements on the validation dataset\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                   f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "            \n",
    "    def predict(self, text_batch):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, 1) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, 1)\n",
    "        \"\"\"\n",
    "        preds = self.forward(text_batch)\n",
    "        return torch.argmax(preds, dim=2)\n",
    "    \n",
    "    def evaluate(self, iterator):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pad_id = self.tag.vocab.stoi[TAG.pad_token]\n",
    "        for batch in tqdm(iterator):\n",
    "            words = batch.text\n",
    "            tags = batch.tag\n",
    "            tags_pred = self.predict(words)\n",
    "            mask = tags.ne(pad_id)\n",
    "            cor = (tags == tags_pred)[mask]\n",
    "            correct += cor.float().sum().item()\n",
    "            total += mask.float().sum().item()\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18af80a2",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Now train your tagger on the training and validation set.\n",
    "Run the cell below to train an RNN, and evaluate it. A proper implementation should reach about **95%+ accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9ead249",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcfbd3cfe4764b1a9db02431f32a73f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5b06143e404b058d3608dd7ee05550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 431.5767 Validation accuracy: 0.8513\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a16d21b83174f67a2830044f29485e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce89a4e96b274f30bafdc51c1c2d2890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 145.9657 Validation accuracy: 0.9171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b407f0044cda487fb46505adcd42fd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de60baa06ea4d2c949776580db89fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 87.0648 Validation accuracy: 0.9389\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca264bbf3ed94212afc1b20ef609b754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facc5371fcb948849d7dbc32fe8ade8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 61.7343 Validation accuracy: 0.9489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8563c5566a4a458f87c516172eb059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6d3ba58a7e4a549ef663e2eb93e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 47.4895 Validation accuracy: 0.9550\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aa51c0d8e2472d86029ed45f4b5737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d79bf0e615474da9e3fe5e10618e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 38.3225 Validation accuracy: 0.9616\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e30289ce8d1478a9b21e4e1626cb684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdffadb785ea46a3ae7980ed35f179ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 32.0916 Validation accuracy: 0.9657\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef88681d88e428996c65848b1b8189c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c2ad88d62d4802be0b3363ccf14ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 27.5518 Validation accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43154b6ea3a3479989ba15641fc2123e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faa63c6dad24ea0b2f0172f01649b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 24.1495 Validation accuracy: 0.9693\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb368eb84d24c2e8e7d55240ed83a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c73aae02c1a47e8b2e315f2affc92c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 21.5062 Validation accuracy: 0.9717\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9bbf54963644e5b1ce1fc0b5d9bd1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d2ada3ab43431c914ea6d488e9197f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.984\n",
      "Test accuracy:     0.972\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "rnn_tagger = RNNTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n",
    "rnn_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n",
    "rnn_tagger.load_state_dict(rnn_tagger.best_model)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {rnn_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {rnn_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a492b",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# LSTM for slot filling\n",
    "\n",
    "Did your RNN perform better than HMM? How much better was it? Was that expected? \n",
    "\n",
    "RNNs tend to exhibit the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem). To remedy this, the Long-Short Term Memory (LSTM) model was introduced. In PyTorch, we can simply use [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). \n",
    "\n",
    "In this section, you'll implement an LSTM model for slot filling. If you've got the RNN model well implemented, this should be extremely straightforward. Just copy and paste your solution, change the call to `nn.RNN` to a call to `nn.LSTM`, and make any other minor adjustments that are necessary. In particular, LSTMs have _two_ recurrent parts, `h` and `c`. You'll thus need to initialize both of these when performing forward computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a38b4def",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__ (self, text, tag, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.text = text\n",
    "        self.text_size = len(text.vocab.itos)\n",
    "        \n",
    "        self.tag = tag\n",
    "        self.tag_size = len(tag.vocab.itos)\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(self.text_size, self.embedding_size) # Lookup layer\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, bidirectional=True)\n",
    "        self.hidden2output = nn.Linear(2 * self.hidden_size, self.tag_size)\n",
    "        \n",
    "        pad_id = self.tag.vocab.stoi[self.tag.pad_token]\n",
    "        self.loss_function = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_id)\n",
    "    \n",
    "    def init_parameters(self, init_low=-0.15, init_high=0.15):\n",
    "        \"\"\"Initialize parameters. We usually use larger initial values for smaller models.\n",
    "        See http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf for a more\n",
    "        in-depth discussion.\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.data.uniform_(init_low, init_high)\n",
    "    \n",
    "    def forward(self, text_batch):\n",
    "        \"\"\"Performs forward, returns logits.\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, 1)\n",
    "        Returns:\n",
    "          logits: a tensor of size (seq_len, 1, self.tag_size)\n",
    "        \"\"\"\n",
    "        word_embeddings = self.word_embeddings(text_batch) # seq_len, 1, embedding_size\n",
    "        # h0, c0 initialized to zeros when nothing inputted\n",
    "        output, (hn, cn) = self.lstm(word_embeddings)\n",
    "        logits = self.hidden2output(output)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def compute_loss(self, logits, tags):\n",
    "        return self.loss_function(logits.view(-1, self.tag_size), tags.view(-1))\n",
    "\n",
    "    def train_all(self, train_iter, val_iter, epochs=10, learning_rate=0.001):\n",
    "        # Switch the module to training mode\n",
    "        self.train()\n",
    "        # Use Adam to optimize the parameters\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        best_validation_accuracy = -float('inf')\n",
    "        best_model = None\n",
    "        # Run the optimization for multiple epochs\n",
    "        for epoch in range(epochs): \n",
    "            total = 0\n",
    "            running_loss = 0.0\n",
    "            for batch in tqdm(train_iter):\n",
    "                # Zero the parameter gradients\n",
    "                self.zero_grad()\n",
    "\n",
    "                # Input and target\n",
    "                words = batch.text # seq_len, 1\n",
    "                tags = batch.tag   # seq_len, 1\n",
    "\n",
    "                # Run forward pass and compute loss along the way.\n",
    "                logits = self.forward(words)\n",
    "                loss = self.compute_loss(logits, tags)\n",
    "\n",
    "                # Perform backpropagation\n",
    "                (loss/words.size(1)).backward()\n",
    "\n",
    "                # Update parameters\n",
    "                optim.step()\n",
    "\n",
    "                # Training stats\n",
    "                total += 1\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # Evaluate and track improvements on the validation dataset\n",
    "            validation_accuracy = self.evaluate(val_iter)\n",
    "            if validation_accuracy > best_validation_accuracy:\n",
    "                best_validation_accuracy = validation_accuracy\n",
    "                self.best_model = copy.deepcopy(self.state_dict())\n",
    "            epoch_loss = running_loss / total\n",
    "            print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n",
    "                   f'Validation accuracy: {validation_accuracy:.4f}')\n",
    "            \n",
    "    def predict(self, text_batch):\n",
    "        \"\"\"Returns the most likely sequence of tags for a sequence of words in `text_batch`.\n",
    "        Arguments: \n",
    "          text_batch: a tensor containing word ids of size (seq_len, 1) \n",
    "        Returns:\n",
    "          tag_batch: a tensor containing tag ids of size (seq_len, 1)\n",
    "        \"\"\"\n",
    "        preds = self.forward(text_batch)\n",
    "        return torch.argmax(preds, dim=2)\n",
    "        \n",
    "    def evaluate(self, iterator):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        pad_id = self.tag.vocab.stoi[TAG.pad_token]\n",
    "        for batch in tqdm(iterator):\n",
    "            words = batch.text\n",
    "            tags = batch.tag\n",
    "            tags_pred = self.predict(words)\n",
    "            mask = tags.ne(pad_id)\n",
    "            cor = (tags == tags_pred)[mask]\n",
    "            correct += cor.float().sum().item()\n",
    "            total += mask.float().sum().item()\n",
    "        return correct/total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9e336",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "Run the cell below to train an LSTM, and evaluate it. A proper implementation should reach about **95%+ accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc3d2af8",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c777af5d7bc4e5cb04288ed9ace5624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d59155562304468afed8d9a98a95353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 512.6145 Validation accuracy: 0.7982\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5873b567e84ecabc6c6bddb64775da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f76b766d914c80bb92cbcc34253551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 197.6441 Validation accuracy: 0.8939\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba33424c444240da95de912aaff40591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05edca5760a4f1fa127ed9c9553969b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 Loss: 115.9406 Validation accuracy: 0.9274\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743f4e85b984468a9ad3b6fb770afc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cbff902d3247a69e94a61c525cf105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 Loss: 79.8286 Validation accuracy: 0.9425\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18c392bdd9649578938fd383689adb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3e90234f0943adac3b556f2e1e8e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 Loss: 60.4330 Validation accuracy: 0.9525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6047d44a4b74baa925c57fb034dfc82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15f885d061a4760be78f73b84cc6236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 Loss: 47.9358 Validation accuracy: 0.9584\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2db73159e0d4119ae5b16ebf2b1407f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02231a49565342edafcd63f399b400c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 Loss: 39.0385 Validation accuracy: 0.9638\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45b1d155c014aac8d79bc7f67ca5c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991cd81f7d67485c94177ba6d988d673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 Loss: 32.5510 Validation accuracy: 0.9678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71aeb91cc4824613a59a0a10f43dfbd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b9c93101244276a806e52e791e1bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 Loss: 27.4036 Validation accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776ce71916a1458bb41add2de15f2305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313e0aad3ade4c4bb4a111f262e1c8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 Loss: 23.5367 Validation accuracy: 0.9745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e15a57980c4a4ed8bd5db717b34b4fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/214 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1b75e2efab402898a2333ef707abcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.984\n",
      "Test accuracy:     0.971\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "lstm_tagger = LSTMTagger(TEXT, TAG, embedding_size=36, hidden_size=36).to(device)\n",
    "lstm_tagger.train_all(train_iter, val_iter, epochs=10, learning_rate=0.001)\n",
    "lstm_tagger.load_state_dict(lstm_tagger.best_model)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {lstm_tagger.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {lstm_tagger.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7da8aa",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# (Optional) Goal 4: Compare HMM to RNN/LSTM with different amounts of training data\n",
    "\n",
    "Vary the amount of training data and compare the performance of HMM to RNN or LSTM (Since RNN is similar to LSTM, picking one of them is enough.) Discuss the pros and cons of HMM and RNN/LSTM based on your experiments.\n",
    "\n",
    "> This part is more open-ended. We're looking for thoughtful experiments and analysis of the results, not any particular result or conclusion.\n",
    "\n",
    "The code below shows how to subsample the training set with downsample ratio `ratio`. To speedup evaluation we only use 50 test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "223a8bc6",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "ratio = 0.1\n",
    "test_size = 50\n",
    "\n",
    "# Set random seeds to make sure subsampling is the same for HMM and RNN\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "train, val, test = tt.datasets.SequenceTaggingDataset.splits(\n",
    "            fields=fields, \n",
    "            path='./data/', \n",
    "            train='atis.train.txt', \n",
    "            validation='atis.dev.txt',\n",
    "            test='atis.test.txt')\n",
    "\n",
    "# Subsample\n",
    "random.shuffle(train.examples)\n",
    "train.examples = train.examples[:int(math.floor(len(train.examples)*ratio))]\n",
    "random.shuffle(test.examples)\n",
    "test.examples = test.examples[:test_size]\n",
    "\n",
    "# Rebuild vocabulary\n",
    "TEXT.build_vocab(train.text, min_freq=MIN_FREQ)\n",
    "TAG.build_vocab(train.tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baea64dd",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b02ebc4",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e72ebca9",
   "metadata": {
    "colab": {},
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa45bd9",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28103faa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n",
    "\n",
    "* Was the project segment clear or unclear? Which portions?\n",
    "* Were the readings appropriate background for the project segment? \n",
    "* Are there additions or changes you think would make the project segment better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08522b",
   "metadata": {},
   "source": [
    "The project segment was very clear especially because we had training from the labs and the code from the labs, so this project was more about understanding the code we didn't modify during the lab and applying to solve similar but somewhat different problems. The readings were appropriate and interesting. I wouldn't change anything to the lab. Maybe I would include in the MarkDown the viterbi algorithm's pseudocode again here but we can simply go look it up at the lab and we also had to problem solve to do it in a vectorized form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da63512",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Instructions for submission of the project segment\n",
    "\n",
    "This project segment should be submitted to Gradescope at <http://go.cs187.info/project2-submit-code> and <http://go.cs187.info/project2-submit-pdf>, which will be made available some time before the due date.\n",
    "\n",
    "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion. You should submit your code to Gradescope at the code submission assignment at <http://go.cs187.info/project2-submit-code>.\n",
    "\n",
    "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope at <http://go.cs187.info/project2-submit-pdf>."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "project2_sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "CS187 Project Segment 2: Sequence labeling â€“ The slot filling task"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
