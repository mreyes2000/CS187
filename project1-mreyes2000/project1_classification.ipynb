{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c9f814",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed with errors. Exit status: 256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement os (from versions: none)\n",
      "ERROR: No matching distribution found for os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs187-2021/project1.git .tmp\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d06d2ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79492b47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f7a12",
   "metadata": {},
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea66096",
   "metadata": {
    "colab_type": "text",
    "id": "JyMdovPAKAHU",
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# CS187\n",
    "\n",
    "## Project segment 1: Text classification\n",
    "\n",
    "In this project segment you will build several varieties of text classifiers using PyTorch.\n",
    "\n",
    "1. A majority baseline.\n",
    "2. A naive Bayes classifer.\n",
    "3. A logistic regression (single-layer perceptron) classifier.\n",
    "4. A multilayer perceptron classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d82b68",
   "metadata": {},
   "source": [
    "# Preparation {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df2e7ba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "import wget\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.legacy as tt\n",
    "\n",
    "from collections import Counter\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d264a05",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdi-spgB0sEi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Random seed\n",
    "random_seed = 1234\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "## GPU check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1646f415",
   "metadata": {
    "colab_type": "text",
    "id": "drbeoB66kJLd"
   },
   "source": [
    "# The task: Answer types for ATIS queries\n",
    "\n",
    "For this and future project segments, you will be working with a standard natural-language-processing dataset, the [ATIS (Airline Travel Information System) dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). This dataset is composed of queries about flights â€“ their dates, times, locations, airlines, and the like.\n",
    "\n",
    "Over the years, the dataset has been annotated in all kinds of ways, with parts of speech, informational chunks, parse trees, and even corresponding SQL database queries. You'll use various of these annotations in future assignments. For this project segment, however, you'll pursue an easier classification task: **given a query, predict the answer type**.\n",
    "\n",
    "These queries ask for different types of answers, such as\n",
    "\n",
    "* Flight IDs: \"Show me the flights from Washington to Boston\"\n",
    "* Fares: \"How much is the cheapest flight to Milwaukee\"\n",
    "* City names: \"Where does flight 100 fly to?\"\n",
    "\n",
    "In all, there are some 30 answer types to the queries.\n",
    "\n",
    "Below is an example taken from this dataset:\n",
    "\n",
    "_Query:_\n",
    "\n",
    "```\n",
    "show me the afternoon flights from washington to boston\n",
    "```\n",
    "\n",
    "_SQL:_\n",
    "\n",
    "```\n",
    "SELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 \n",
    "   WHERE flight_1.departure_time BETWEEN 1200 AND 1800 \n",
    "     AND ( flight_1.from_airport = airport_service_1.airport_code \n",
    "           AND airport_service_1.city_code = city_1.city_code \n",
    "           AND city_1.city_name = 'WASHINGTON' \n",
    "           AND flight_1.to_airport = airport_service_2.airport_code \n",
    "           AND airport_service_2.city_code = city_2.city_code \n",
    "           AND city_2.city_name = 'BOSTON' )\n",
    "```\n",
    "\n",
    "In this project segment, we will consider the answer type for a natural-language query to be the target field of the corresponding SQL query. For the above example, the answer type would be *flight_id*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a209120",
   "metadata": {
    "colab_type": "text",
    "id": "drbeoB66kJLd"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "> Read over this section, executing the cells, and **making sure you understand what's going on before proceeding to the next parts.**\n",
    "\n",
    "First, let's download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49834610",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGVWcvlk080Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................] 250477 / 250477"
     ]
    }
   ],
   "source": [
    "data_dir = \"https://raw.githubusercontent.com/nlp-course/data/master/ATIS/\"\n",
    "os.makedirs('data', exist_ok=True)\n",
    "for file in [\"train.nl\",\n",
    "             \"train.sql\",\n",
    "             \"dev.nl\",\n",
    "             \"dev.sql\",\n",
    "             \"test.nl\",\n",
    "             \"test.sql\"]:\n",
    "    wget.download(f\"{data_dir}{file}\", out='data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed6e38",
   "metadata": {
    "colab_type": "text",
    "id": "tPJ6ihGH1Oz8"
   },
   "source": [
    "We use `torchtext` to prepare the data, as in lab 1-5. More information on `torchtext` can be found at https://pytorch.org/text/0.8.1/data.html.\n",
    "\n",
    "> You'll notice that we link to version 0.8.1 of the PyTorch documentation, because the `torchtext.data.Field` class is now deprecated. We therefore imported it as `torchtext.legacy` at the top of this notebook. Sadly, torchtext has no convenient replacement for `Field` at the moment.\n",
    "\n",
    "To begin, `torchtext` requires that we define a mapping from the raw data to featurized indices, called a [`Field`](https://torchtext.readthedocs.io/en/latest/data.html#fields). We need one field for processing the question (`TEXT`), and another for processing the label (`LABEL`). These fields make it easy to map back and forth between readable data and lower-level representations like numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e122a7d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0XvgL1X6GiK"
   },
   "outputs": [],
   "source": [
    "TEXT = tt.data.Field(lower=True,            # lowercase all tokens\n",
    "                     sequential=True,       # sequential data\n",
    "                     include_lengths=False, # do not include lengths\n",
    "                     batch_first=True,      # batches will be batch_size X max_len\n",
    "                     tokenize=tt.data.get_tokenizer(\"basic_english\")) \n",
    "LABEL = tt.data.Field(batch_first=True, sequential=False, unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6b763",
   "metadata": {
    "colab_type": "text",
    "id": "fTkktPQa13yJ"
   },
   "source": [
    "We provide an interface for loading ATIS data, built on top of [`torchtext.data.Dataset`](https://pytorch.org/text/data.html#torchtext.data.Dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b5f7ae",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiZRD-ua1Jfm"
   },
   "outputs": [],
   "source": [
    "class ATIS(tt.data.Dataset):\n",
    "  @staticmethod\n",
    "  def sort_key(ex):\n",
    "    return len(ex.text)\n",
    "\n",
    "  def __init__(self, path, text_field, label_field, **kwargs):\n",
    "    \"\"\"Creates an ATIS dataset instance given a path and fields.\n",
    "    Arguments:\n",
    "        path: Path to the data file\n",
    "        text_field: The field that will be used for text data.\n",
    "        label_field: The field that will be used for label data.\n",
    "        Remaining keyword arguments: Passed to the constructor of\n",
    "            tt.data.Dataset.\n",
    "    \"\"\"\n",
    "    fields = [('text', text_field), ('label', label_field)]\n",
    "    \n",
    "    examples = []\n",
    "    # Get text\n",
    "    with open(path+'.nl', 'r') as f:\n",
    "        for line in f:\n",
    "            ex = tt.data.Example()\n",
    "            ex.text = text_field.preprocess(line.strip()) \n",
    "            examples.append(ex)\n",
    "    \n",
    "    # Get labels\n",
    "    with open(path+'.sql', 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            label = self._get_label_from_query(line.strip())\n",
    "            examples[i].label = label\n",
    "            \n",
    "    super(ATIS, self).__init__(examples, fields, **kwargs)\n",
    "  \n",
    "  def _get_label_from_query(self, query):\n",
    "    \"\"\"Returns the answer type from `query` by dead reckoning.\n",
    "    It's basically the second or third token in the SQL query.\n",
    "    \"\"\"    \n",
    "    match = re.match(r'\\s*SELECT\\s+(DISTINCT\\s*)?(\\w+\\.)?(?P<label>\\w+)', query)\n",
    "    if match:\n",
    "        label = match.group('label')\n",
    "    else:\n",
    "        raise RuntimeError(f'no label in query {query}')\n",
    "    return label\n",
    "\n",
    "  @classmethod\n",
    "  def splits(cls, text_field, label_field, path='./',\n",
    "              train='train', validation='dev', test='test',\n",
    "              **kwargs):\n",
    "    \"\"\"Create dataset objects for splits of the ATIS dataset.\n",
    "    \n",
    "    Arguments:\n",
    "        text_field: The field that will be used for the sentence.\n",
    "        label_field: The field that will be used for label data.\n",
    "        root: The root directory that the dataset's zip archive will be\n",
    "            expanded into; therefore the directory in whose trees\n",
    "            subdirectory the data files will be stored.\n",
    "        train: The filename of the train data. Default: 'train.txt'.\n",
    "        validation: The filename of the validation data, or None to not\n",
    "            load the validation set. Default: 'dev.txt'.\n",
    "        test: The filename of the test data, or None to not load the test\n",
    "            set. Default: 'test.txt'.\n",
    "        Remaining keyword arguments: Passed to the splits method of\n",
    "            Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    train_data = None if train is None else cls(\n",
    "        os.path.join(path, train), text_field, label_field, **kwargs)\n",
    "    val_data = None if validation is None else cls(\n",
    "        os.path.join(path, validation), text_field, label_field, **kwargs)\n",
    "    test_data = None if test is None else cls(\n",
    "        os.path.join(path, test), text_field, label_field, **kwargs)\n",
    "    return tuple(d for d in (train_data, val_data, test_data)\n",
    "                   if d is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f461a6",
   "metadata": {
    "colab_type": "text",
    "id": "bGx_0Dk_d7fc"
   },
   "source": [
    "We split the data into training, validation, and test corpora, and build the vocabularies from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a48cfd",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aetkuF_1d1nb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocab: 512\n",
      "Number of labels: 30\n"
     ]
    }
   ],
   "source": [
    "# Make splits for data\n",
    "train_data, val_data, test_data = ATIS.splits(TEXT, LABEL, path='./data/')\n",
    "\n",
    "# Build vocabulary for data fields\n",
    "MIN_FREQ = 3 # words appearing fewer than 3 times are treated as 'unknown'\n",
    "TEXT.build_vocab(train_data, min_freq=MIN_FREQ)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# Compute size of vocabulary\n",
    "vocab_size = len(TEXT.vocab.itos)\n",
    "num_labels = len(LABEL.vocab.itos)\n",
    "print(f\"Size of vocab: {vocab_size}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b913c",
   "metadata": {},
   "source": [
    "To get a sense of the kinds of things that are asked about in this dataset, here is the list of all of the answer types in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "687badd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 advance_purchase\n",
      " 1 aircraft_code\n",
      " 2 airline_code\n",
      " 3 airport_code\n",
      " 4 airport_location\n",
      " 5 arrival_time\n",
      " 6 basic_type\n",
      " 7 booking_class\n",
      " 8 city_code\n",
      " 9 city_name\n",
      "10 count\n",
      "11 day_name\n",
      "12 departure_time\n",
      "13 fare_basis_code\n",
      "14 fare_id\n",
      "15 flight_id\n",
      "16 flight_number\n",
      "17 ground_fare\n",
      "18 meal_code\n",
      "19 meal_description\n",
      "20 miles_distant\n",
      "21 minimum_connect_time\n",
      "22 minutes_distant\n",
      "23 restriction_code\n",
      "24 state_code\n",
      "25 stop_airport\n",
      "26 stops\n",
      "27 time_elapsed\n",
      "28 time_zone_code\n",
      "29 transport_type\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(sorted(LABEL.vocab.itos)):\n",
    "    print(f\"{i:2d} {label}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42d6e4",
   "metadata": {
    "colab_type": "text",
    "id": "5HCgGp4ACIvL"
   },
   "source": [
    "## Handling unknown words\n",
    "\n",
    "Note that we mapped words appearing fewer than 3 times to a special _unknown_ token (we're using the `torchtext` default, `<unk>`) for two reasons: \n",
    "\n",
    "1. Due to the scarcity of such rare words in training data, we might not be able to learn generalizable conclusions about them.\n",
    "2. Introducing an unknown token allows us to deal with out-of-vocabulary words in the test data as well: we just map those words to `<unk>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb5076c8",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tr5Omf6yBTsI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown token: <unk>\n",
      "Unknown token id: 0\n",
      "An unknown token: IAmAnUnknownWordForSure\n",
      "Mapped back to word id: 0\n",
      "Mapped to <unk>?: True\n"
     ]
    }
   ],
   "source": [
    "unk_token = TEXT.unk_token\n",
    "print (f\"Unknown token: {unk_token}\")\n",
    "unk_index = TEXT.vocab.stoi[unk_token]\n",
    "print (f\"Unknown token id: {unk_index}\")\n",
    "\n",
    "# UNK example\n",
    "example_unk_token = 'IAmAnUnknownWordForSure'\n",
    "print (f\"An unknown token: {example_unk_token}\")\n",
    "print (f\"Mapped back to word id: {TEXT.vocab.stoi[example_unk_token]}\")\n",
    "print (f\"Mapped to <unk>?: {TEXT.vocab.stoi[example_unk_token] == unk_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe823bf7",
   "metadata": {
    "colab_type": "text",
    "id": "go2q9-vd6RO7"
   },
   "source": [
    "## Batching the data\n",
    "\n",
    "To load data in batches, we use `data.BucketIterator`. This enables us to iterate over the dataset under a given `BATCH_SIZE` which specifies how many examples we want to process at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd05861d",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QBVa2Krb5IcY"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter = tt.data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\n",
    "val_iter = tt.data.BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\n",
    "test_iter = tt.data.Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0c2c02",
   "metadata": {
    "colab_type": "text",
    "id": "UBpXYa4h5g5I"
   },
   "source": [
    "Let's look at a single batch from one of these iterators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "456434e0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r6QHKuQ75bjd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of text batch: torch.Size([32, 31])\n",
      "Third sentence in batch: tensor([ 36,  29,   5,  68,  30,   4,   3, 126,   2,  96,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1])\n",
      "Converted back to string: please list the american airlines flights from houston to milwaukee <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Size of label batch: torch.Size([32])\n",
      "Third label in batch: 0\n",
      "Converted back to string: flight_id\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "text = batch.text\n",
    "print (f\"Size of text batch: {text.size()}\")\n",
    "print (f\"Third sentence in batch: {text[2]}\")\n",
    "print (f\"Converted back to string: {' '.join([TEXT.vocab.itos[i] for i in text[2]])}\")\n",
    "\n",
    "label = batch.label\n",
    "print (f\"Size of label batch: {label.size()}\")\n",
    "print (f\"Third label in batch: {label[2]}\")\n",
    "print (f\"Converted back to string: {LABEL.vocab.itos[label[2].item()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f03240",
   "metadata": {
    "colab_type": "text",
    "id": "5iL-spuDoLDt"
   },
   "source": [
    "You might notice some padding tokens `<pad>` when we convert word ids back to strings, or equivalently, padding ids `1` in the corresponding tensor. The reason why we need such padding is because the sentences in a batch might be of different lengths, and to save them in a 2D tensor for parallel processing, sentences that are shorter than the longest sentence need to be padded with some placeholder values. `torchtext` does all this for us automatically. Later during training you'll need to make sure that the paddings do not affect the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53d27ff",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifE_-aPo81x7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding token: <pad>\n",
      "Padding word id: 1\n"
     ]
    }
   ],
   "source": [
    "padding_token = TEXT.pad_token\n",
    "print (f\"Padding token: {padding_token}\")\n",
    "\n",
    "padding_id = TEXT.vocab.stoi[padding_token]\n",
    "print (f\"Padding word id: {padding_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eee8c6",
   "metadata": {
    "colab_type": "text",
    "id": "H6rFmJcj-rgl"
   },
   "source": [
    "Alternatively, we can also directly iterate over the individual examples in `train_data`, `val_data` and `test_data`. Here the returned values are the raw sentences and labels instead of their corresponding ids, and you might need to explicitly deal with the unknown words, unlike using bucket iterators which automatically map unknown words to an unknown word id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b0218c4",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q2GGBhTF-5p0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_id  -- list all the flights that arrive at general mitchell international from various cities\n",
      "flight_id  -- give me the flights leaving denver august ninth coming back to boston\n",
      "flight_id  -- what flights from tacoma to orlando on saturday\n",
      "fare_id    -- what is the most expensive one way fare from boston to atlanta on american airlines\n",
      "flight_id  -- what flights return from denver to philadelphia on a saturday\n"
     ]
    }
   ],
   "source": [
    "for example in train_iter.dataset[:5]: # train_iter.dataset is just train_data\n",
    "    print(f\"{example.label:10} -- {' '.join(example.text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d97a9",
   "metadata": {},
   "source": [
    "## Notations used\n",
    "\n",
    "In this project segment, we'll use the following notations.\n",
    "\n",
    "* Sequences of elements (vectors and the like) are written with angle brackets and commas ($\\langle w_1, \\ldots, w_M \\rangle$) or directly with no punctuation ($w_1 \\cdots w_M$).\n",
    "* Sets are notated similarly but with braces, ($\\{ v_1, \\ldots, v_V \\}$).\n",
    "* Maximum indices ($M$, $N$, $V$, $T$, and $X$ in the following) are written as uppercase italics.\n",
    "* Variables over sequences and sets are written in boldface ($\\vect{w}$), typically with the same letter as the variables over their elements.\n",
    "\n",
    "In particular,\n",
    "\n",
    "* $\\vect{w} = w_1 \\cdots w_M$: A text to be classified, each element $w_j$ being a word token.\n",
    "* $\\vect{v} = \\{ v_1, \\ldots, v_V\\}$: A vocabulary, each element $v_k$ being a word type.\n",
    "* $\\vect{x} = \\langle x_1, \\ldots, x_X \\rangle$: Input features to a model.\n",
    "* $\\vect{y} = \\{ y_1, \\ldots, y_N \\}$: The output classes of a model, each element $y_i$ being a class label.\n",
    "* $\\vect{T} = \\langle \\vect{w}^{(1)}, \\ldots, \\vect{w}^{(T)} \\rangle$: The training corpus of texts.\n",
    "* $\\vect{Y} = \\langle y^{(1)}, \\ldots, y^{(T)} \\rangle$: The corresponding gold labels for the training examples in $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422f78e",
   "metadata": {
    "colab_type": "text",
    "id": "CxDMbHJG9Qpg"
   },
   "source": [
    "# To Do: Establish a majority baseline\n",
    "\n",
    "A simple baseline for classification tasks is to always predict the most common class. \n",
    "Given a training set of texts $\\vect{T}$ labeled by classes $\\vect{Y}$, we classify an input text $\\vect{w} = w_1 \\cdots w_M$ as the class $y_i$ that occurs most frequently in the training data, that is, specified by\n",
    "\n",
    "$$ \\argmax{i} \\cnt{y_i} $$\n",
    "\n",
    "and thus ignoring the input entirely (!).\n",
    "\n",
    "**Implement the majority baseline and compute test accuracy using the starter code below.** For this baseline, and for the naive Bayes classifier later, we don't need to use the validation set since we don't tune any hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54ffa36b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hd8XvBof6rVa"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "def majority_baseline_accuracy(train_iter, test_iter):\n",
    "    \"\"\"Returns the most common label in the training set, and the accuracy of\n",
    "    the majority baseline on the test set.\n",
    "    \"\"\"\n",
    "    # Find the most common label using a counter\n",
    "    train_counter = Counter(train_iter.dataset.label)\n",
    "    most_common_label = train_counter.most_common(1)[0][0]\n",
    "    \n",
    "    # Compute test accuracy\n",
    "    test_counter = Counter(test_iter.dataset.label)\n",
    "    test_accuracy = test_counter[most_common_label] / sum(test_counter.values())\n",
    "    return most_common_label, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357cc720",
   "metadata": {},
   "source": [
    "How well does your classifier work? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dde3dc4",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOC7A_34v1zB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common label: flight_id\n",
      "Test accuracy:     0.683\n"
     ]
    }
   ],
   "source": [
    "# Call the method to establish a baseline\n",
    "most_common_label, test_accuracy = majority_baseline_accuracy(train_iter, test_iter)\n",
    "\n",
    "print(f'Most common label: {most_common_label}\\n'\n",
    "      f'Test accuracy:     {test_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26817ff0",
   "metadata": {},
   "source": [
    "# Additional modules and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfdbbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def bag_of_words(batch, dim1, dim2):\n",
    "    # Initialize zeros\n",
    "    X = torch.zeros((dim1, dim2))\n",
    "    for idx, example in enumerate(batch):\n",
    "        for i in example:\n",
    "            # Ignore <pad>\n",
    "            if i != padding_id:\n",
    "                X[idx][int(i)] += 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b200d5e",
   "metadata": {
    "colab_type": "text",
    "id": "dNbq_QvG_XGY"
   },
   "source": [
    "# To Do: Implement a Naive Bayes classifier\n",
    "\n",
    "\n",
    "## Review of the naive Bayes method\n",
    "\n",
    "Recall from lab 1-3 that the Naive Bayes classification method classifies a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ as the class $y_i$ given by the following maximization:\n",
    "\n",
    "$$\n",
    "\\argmax{i} \\Prob(y_i \\given \\vect{w}) \\approx \\argmax{i} \\Prob(y_i) \\cdot \\prod_{j=1}^M \\Prob(w_j \\given y_i)\n",
    "$$\n",
    "\n",
    "or equivalently (since taking the log is monotonic)\n",
    "\n",
    "\\begin{align}\n",
    "\\argmax{i} \\Prob(y_i \\given \\vect{w}) &= \\argmax{i} \\log\\Prob(y_i \\given \\vect{w}) \\\\\n",
    "&\\approx \\argmax{i} \\left(\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\\right)\n",
    "\\end{align}\n",
    "\n",
    "All we need, then, to apply the Naive Bayes classification method is values for the various log probabilities: the priors $\\log\\Prob(y_i)$ and the likelihoods $\\log\\Prob(w_j \\given y_i)$, for each feature (word) $w_j$ and each class $y_i$.\n",
    "\n",
    "We can estimate the prior probabilities $\\Prob(y_i)$ by examining the empirical probability in the training set. That is, we estimate \n",
    "\n",
    "$$ \\Prob(y_i) \\approx \\frac{\\cnt{y_i}}{\\sum_j \\cnt{y_j}} $$\n",
    "\n",
    "We can estimate the likelihood probabilities $\\Prob(w_j \\given y_i)$ similarly by examining the empirical probability in the training set. That is, we estimate \n",
    "\n",
    "$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i}}{\\sum_{j'} \\cnt{w_{j'}, y_i}} $$\n",
    "\n",
    "To allow for cases in which the count $\\cnt{w_j, y_i}$ is zero, we can use a modified estimate incorporating add-$\\delta$ smoothing:\n",
    "\n",
    "$$ \\Prob(w_j \\given y_i) \\approx \\frac{\\cnt{w_j, y_i} + \\delta}{\\sum_{j'} \\cnt{w_{j'}, y_i} + \\delta \\cdot V} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a64ca1",
   "metadata": {
    "colab_type": "text",
    "id": "dNbq_QvG_XGY"
   },
   "source": [
    "## Two conceptions of the naive Bayes method implementation\n",
    "\n",
    "We can store all of these parameters in different ways, leading to two different implementation conceptions. We review two conceptions of implementing the naive Bayes classification of a text $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$, corresponding to using different representations of the input $\\vect{x}$ to the model: the index representation and the bag-of-words representation. \n",
    "\n",
    "Within each conception, the parameters of the model will be stored in one or more matrices. The conception dictates what operations will be performed with these matrices.\n",
    "\n",
    "### Using the index representation\n",
    "\n",
    "In the first conception, we take the input elements $\\vect{x} = \\langle x_1, x_2, \\ldots, x_M \\rangle$ to be the _vocabulary indices_ of the words $\\vect{w} = w_1 \\cdots w_M$. That is, each word token $w_i$ is of the word type in the vocabulary $\\vect{v}$ at index $x_i$, so \n",
    "\n",
    "$$ v_{x_i} = w_i $$\n",
    "\n",
    "In this representation, the input vector has the same length as the word sequence.\n",
    "\n",
    "We think of the likelihood probabilities as forming a matrix, call it $\\vect{L}$, where the $i,j$-th element stores $\\log \\Prob(v_j \\given y_i)$. \n",
    "\n",
    "$$\\vect{L}_{ij} = \\log\\Prob(v_j \\given y_i)$$\n",
    "\n",
    "Similarly, for the priors, we'll have \n",
    "\n",
    "$$\\vect{P}_{i} = \\log\\Prob(y_i)$$\n",
    "\n",
    "Now the maximization can be implemented as \n",
    "\n",
    "\\begin{align}\n",
    "\\argmax{i} \\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)\n",
    "&= \\argmax{i} \\vect{P}_i + \\sum_{j=1}^M \\vect{L}_{x_j i}\n",
    "\\end{align}\n",
    "\n",
    "Implemented in this way, we see that the use of each input $x_i$ is as an _index_ into the likelihood matrix. \n",
    "\n",
    "### Using the bag-of-words representation\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/naive-bayes-figure.png\" width=400 align=right />\n",
    "\n",
    "Notice that since each word in the input is treated separately, the order of the words doesn't matter. Rather, all that matters is how frequently each word type occurs in a text. Consequently, we can use the bag-of-words representation introduced in lab 1-1.\n",
    "\n",
    "Recall that the bag-of-words representation of a text is just its frequency distribution over the vocabulary, which we will notate $bow(\\vect{w})$. Given a vocabulary of word types $\\vect{v} = \\langle v_1, v_2, \\ldots, v_V \\rangle$, the representation of a sentence $\\vect{w} = \\langle w_1, w_2, \\ldots, w_M \\rangle$ is a vector $\\vect{x}$ of size $V$, where \n",
    "\n",
    "$$\\begin{aligned}\n",
    "bow(\\vect{w})_j &= \\sum_{i=1}^M 1[w_i = v_j] & \\mbox{for $1 \\leq j \\leq V$}\n",
    "\\end{aligned}$$\n",
    "\n",
    "We write $1[w_i = v_j]$ to indicate 1 if $w_i = v_j$ and 0 otherwise. For convenience, we'll add an extra $(V+1)$-st element to the end of the bag-of-words vector, a single $1$ whose use will be clear shortly. That is,\n",
    "\n",
    "$$bow(\\vect{w})_{V+1} = 1$$\n",
    "\n",
    "Under this conception, then, we'll take the input $\\vect{x}$ to be $bow(\\vect{w})$. Instead of the input having the same length as the text, it has the same length as the vocabulary.\n",
    "\n",
    "As described in lecture, represented in this way, the quantity to be maximized in the naive Bayes method\n",
    "\n",
    "$$\\log\\Prob(y_i) + \\sum_{j=1}^M \\log\\Prob(w_j \\given y_i)$$\n",
    "\n",
    "can be calculated as \n",
    "\n",
    "$$\\log\\Prob(y_i) + \\sum_{j=1}^V x_j \\cdot \\log\\Prob(v_j \\given y_i)$$\n",
    "\n",
    "which is just $\\vect{U} \\vect{x}$ for a suitable choice of $N \\times (V+1)$ matrix $\\vect{U}$, namely\n",
    "\n",
    "$$ \\vect{U}_{ij} = \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "        \\log \\Prob(v_j \\given y_i) & \\mbox{$1 \\leq i \\leq N$ and $1 \\leq j \\leq V$} \\\\\n",
    "        \\log \\Prob(y_i) & \\mbox{$1 \\leq i \\leq N$ and $j = V+1$} \n",
    "    \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "Under this implementation conception, we've reduced naive Bayes calculations to a single matrix operation. This conception is depicted in the figure at right.\n",
    "\n",
    "You are free to use either conception in your implementation of naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2c494",
   "metadata": {
    "colab_type": "text",
    "id": "bJDEXnaESogl"
   },
   "source": [
    "## Implement the naive Bayes classifier\n",
    " \n",
    "For the implementation, we ask you to implement a Python class `NaiveBayes` that will have (at least) the following three methods:\n",
    "\n",
    "1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n",
    "\n",
    "2. `train`: A method that takes a training data iterator and estimates all of the log probabilities $\\log\\Prob(c_i)$ and $\\log\\Prob(x_j \\given c_i)$ as described above. Perform add-$\\delta$ smoothing with $\\delta=1$. These parameters will be used by the `evaluate` method to evaluate a test dataset for accuracy, so you'll want to store them in some data structures in objects of the class.\n",
    "\n",
    "3. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n",
    "\n",
    "You can organize your code using either of the conceptions of Naive Bayes described above.\n",
    "\n",
    "You should expect to achieve about an **86% test accuracy** on the ATIS task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c68969fc",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SwSLwSEO2uyw"
   },
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    def __init__ (self, text, label):\n",
    "        self.text = text\n",
    "        self.label = label\n",
    "        self.padding_id = text.vocab.stoi[text.pad_token]\n",
    "        self.V = len(text.vocab.itos) # vocabulary size\n",
    "        self.N = len(label.vocab.itos) # the number of classes\n",
    "        # TODO: Add your code here\n",
    "        self.U_mat = torch.empty((self.N, self.V + 1))\n",
    "        self.delta = 1\n",
    "  \n",
    "    def train(self, iterator):\n",
    "        \"\"\"Calculates and stores log probabilities for training dataset `iterator`.\"\"\"\n",
    "        # TODO: Implement this method.\n",
    "        # Tensors for numerators of priors and likelihoods\n",
    "        num_prior = torch.zeros(self.N, 1)\n",
    "        num_like = torch.zeros((self.N, self.V))\n",
    "        for batch in iterator:\n",
    "            X_bow = bag_of_words(batch.text, len(batch), self.V)\n",
    "            num_like.index_add_(0, batch.label, X_bow)\n",
    "            num_prior.index_add_(0, batch.label, torch.ones(len(batch), 1))\n",
    "            \n",
    "        # Tensors for denominators of priors and likelihoods\n",
    "        denom_like = torch.sum(num_like, dim=1).reshape(-1,1)\n",
    "        denom_prior = torch.sum(num_prior)\n",
    "        \n",
    "        # Compute likelihoods and priors using formula\n",
    "        likelihoods = (num_like + self.delta) / (denom_like + self.V * self.delta)\n",
    "        priors = num_prior / denom_prior\n",
    "        \n",
    "        # Append priors as last column of likelihoods\n",
    "        self.U_mat = torch.log2(torch.cat((likelihoods.T, priors.T))).T\n",
    "\n",
    "    def evaluate(self, iterator):\n",
    "        \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n",
    "        # TODO: Implement this method.\n",
    "        c_num = 0\n",
    "        total = 0\n",
    "        for batch in iterator:\n",
    "            X_bow = bag_of_words(batch.text, len(batch), self.V)\n",
    "            ones = torch.ones(1, len(batch))\n",
    "            \n",
    "            # Add ones to final row of X_bow to multiply last column of U by this new X_bow_pad\n",
    "            X_bow_pad = torch.cat((X_bow.T, ones))\n",
    "            predictions = torch.argmax(torch.matmul(self.U_mat, X_bow_pad), dim=0)\n",
    "            total += predictions.size(0)\n",
    "            c_num += (predictions == batch.label).float().sum().item()\n",
    "        return c_num / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c797174",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOO52qZMv1zS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.897\n",
      "Test accuracy:     0.864\n"
     ]
    }
   ],
   "source": [
    "# Instantiate and train classifier\n",
    "nb_classifier = NaiveBayes(TEXT, LABEL)\n",
    "nb_classifier.train(train_iter)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(f'Training accuracy: {nb_classifier.evaluate(train_iter):.3f}\\n'\n",
    "      f'Test accuracy:     {nb_classifier.evaluate(test_iter):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892fe69",
   "metadata": {
    "colab_type": "text",
    "id": "-pH4ph3uHnvD"
   },
   "source": [
    "# To Do: Implement a logistic regression classifier\n",
    "\n",
    "In this part, you'll complete a PyTorch implementation of a logistic regression (equivalently, a single layer perceptron) classifier. We review logistic regression here highlighting the similarities to the matrix-multiplication conception of naive Bayes. Thus, we take the input $\\vect{x}$ to be the bag-of-words representation $bow(\\vect{w})$. But as before you are free to use either implementation approach.\n",
    "\n",
    "## Review of logistic regression\n",
    "\n",
    "Similar to naive Bayes, in logistic regression, we assign a probability to a text $\\vect{x}$ by merely multiplying an $N \\times V$ matrix $\\vect{U}$ by it. However, we don't stipulate that the values in the matrix $\\vect{U}$ be estimated from the training corpus in the \"naive Bayes\" manner. Instead, we allow them to take on any value, using a training regime to select good values.\n",
    "\n",
    "In order to make sure that the output of the matrix multiplication $\\vect{U}\\vect{x}$ is mapped onto a probability distribution, we apply a nonlinear function to renormalize the values. We use the softmax function, a generalization of the sigmoid function from lab 1-4, defined by \n",
    "\n",
    "$$\\softmax(\\vect{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}$$\n",
    "\n",
    "for each of the indices $i$ from $1$ to $N$.\n",
    "\n",
    "In summary, we model $\\Prob (y \\given \\vect{x})$ as\n",
    "\n",
    "$$ \\Prob(y_i \\given \\vect{x}) = \\softmax ( \\vect{U} \\vect{x} )_i $$\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/logistic-regression-figure.png\" alt=\"logistic regression illustration\" width=\"400\"  align=right />\n",
    "\n",
    "The calculation of $\\Prob(y \\given \\vect{x})$ for each text $\\vect{x}$ is referred to as the _forward_ computation. In summary, the forward computation for logistic regression involves a linear calculation ($\\vect{U} \\vect{x}$) followed by a nonlinear calculation ($\\softmax$). We think of the perceptron (and more generally many of these neural network models) as transforming from one representation to another. A perceptron performs a linear transformation from the index or bag-of-words representation of the text to a representation as a vector, followed by a nonlinear transformation, a softmax or sigmoid, giving a representation as a probability distribution over the class labels. This single-layer perceptron thus involves two _sublayers_. (In the next part of the project segment, you'll experiment with a multilayer perceptron, with two perceptron layers, and hence four sublayers.)\n",
    "\n",
    "The loss function you'll use is the negative log probability $-\\log \\Prob (y \\given \\vect{x})$. The negative is used, since it is convention to minimize loss, whereas we want to maximize log likelihood. \n",
    "\n",
    "The forward and loss computations are illustrated in the figure at right. In practice, for numerical stability reasons, PyTorch absorbs the softmax operation into the loss function `nn.CrossEntropyLoss`. That is, the input to the `nn.CrossEntropyLoss` function is the vector of sums $\\vect{U} \\vect{x}$ (the last step in the box marked \"your job\" in the figure) rather than the vector of probabilities $\\Prob(y \\given \\vect{x})$. That makes things easier for you (!), since you're responsible only for the first sublayer.\n",
    "\n",
    "Given a forward computation, the weights can then be adjusted by taking a step opposite to the gradient of the loss function. Adjusting the weights in this way is referred to as the _backward_ computation. Fortunately, `torch` takes care of the backward computation for you, just as in lab 1-5.\n",
    "\n",
    "The optimization process of performing the forward computation, calculating the loss, and performing the backward computation to improve the weights is done repeatedly until the process converges on a (hopefully) good set of weights. You'll find this optimization process in the `train_all` method that we've provided. The trained weights can then be used to perform classification on a test set. See the `evaluate` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d6290",
   "metadata": {
    "colab_type": "text",
    "id": "KEgCwQVHrVw0"
   },
   "source": [
    "## Implement the logistic regression classifier\n",
    "\n",
    "For the implementation, we ask you to implement a logistic regression classifier as a subclass of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module). You need to implement the following methods:\n",
    "\n",
    "1. `__init__`: an initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n",
    "\n",
    "    During initialization, you'll want to define a [tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor) of weights, wrapped in [`torch.nn.Parameter`](https://pytorch.org/docs/master/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter), [initialized randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_), which plays the role of $\\vect{U}$. The elements of this tensor are the parameters of the `torch.nn` instance in the following special technical sense: It is the parameters of the module whose gradients will be calculated and whose values will be updated. Alternatively, you might find it easier to use the [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) which is a wrapper to the weight tensor with a lookup implementation.\n",
    "\n",
    "2. `forward`: given a text batch of size `batch_size X max_length`, return a tensor of logits of size `batch_size X num_labels`. That is, for each text $\\vect{x}$ in the batch and each label $y$, you'll be calculating $\\vect{U}\\vect{x}$ as shown in the figure, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you won't need to deal with that.\n",
    "\n",
    "3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n",
    "\n",
    "4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n",
    "\n",
    "Some things to consider:\n",
    "\n",
    "1. The parameters of the model, the weights, need to be initialized properly. We suggest initializing them to some small random values. See [`torch.uniform_`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_).\n",
    "\n",
    "2. You'll want to make sure that padding tokens are handled properly. What should the weight be for the padding token?\n",
    "\n",
    "3. In extracting the proper weights to sum up, based on the word types in a sentence, we are essentially doing a lookup operation. You might find [`nn.Embedding`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) or [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather) useful.\n",
    "\n",
    "You should expect to achieve about **90%** accuracy on the ATIS classificiation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2850fb",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dBdFcvg-PYBo"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "  def __init__ (self, text, label):\n",
    "    super().__init__()\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "    self.padding_id = text.vocab.stoi[text.pad_token]\n",
    "    # Keep the vocabulary sizes available\n",
    "    self.N = len(label.vocab.itos) # num_classes\n",
    "    self.V = len(text.vocab.itos)  # vocab_size\n",
    "    # Specify cross-entropy loss for optimization\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    # TODO: Create and initialize a tensor for the weights,\n",
    "    #       or create an nn.Embedding module and initialize\n",
    "    self.U_mat = torch.nn.Parameter(torch.rand((self.N, self.V)) / 1000)\n",
    "\n",
    "  def forward(self, text_batch):\n",
    "    # TODO: Calculate the logits (Ux) for the `text_batch`, \n",
    "    #       returning a tensor of size batch_size x num_labels\n",
    "    X_bow = bag_of_words(text_batch, len(text_batch), self.V)\n",
    "    return torch.matmul(self.U_mat, X_bow.T).T\n",
    "\n",
    "  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n",
    "    # Switch the module to training mode\n",
    "    self.train()\n",
    "    # Use Adam to optimize the parameters\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    best_validation_accuracy = -float('inf')\n",
    "    best_model = None\n",
    "    # Run the optimization for multiple epochs\n",
    "    with tqdm(range(epochs), desc='train', position=0) as pbar:\n",
    "      for epoch in pbar:\n",
    "        c_num = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_iter, desc='batch', leave=False):\n",
    "          # TODO: set labels, compute logits (Ux in this model), \n",
    "          #       loss, and update parameters\n",
    "          ...\n",
    "          optim.zero_grad()\n",
    "          labels = batch.label\n",
    "          logits = self.forward(batch.text)\n",
    "          loss = self.criterion(logits, labels)\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "          ...\n",
    "          # Prepare to compute the accuracy\n",
    "          predictions = torch.argmax(logits, dim=1)\n",
    "          total += predictions.size(0)\n",
    "          c_num += (predictions == labels).float().sum().item()        \n",
    "          running_loss += loss.item() * predictions.size(0)\n",
    "\n",
    "        # Evaluate and track improvements on the validation dataset\n",
    "        validation_accuracy = self.evaluate(val_iter)\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "          best_validation_accuracy = validation_accuracy\n",
    "          self.best_model = copy.deepcopy(self.state_dict())\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = c_num / total\n",
    "        pbar.set_postfix(epoch=epoch+1, loss=epoch_loss, train_acc = epoch_acc, val_acc=validation_accuracy)\n",
    "\n",
    "  def evaluate(self, iterator):\n",
    "    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n",
    "    self.eval()   # switch the module to evaluation mode\n",
    "    # TODO: Compute accuracy\n",
    "    c_num = 0\n",
    "    total = 0\n",
    "    for batch in iterator:\n",
    "        labels = batch.label\n",
    "        logits = self.forward(batch.text)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        total += predictions.size(0)\n",
    "        c_num += (predictions == labels).float().sum().item()\n",
    "    return c_num / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64d93e1b",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6wW6UcFqv1zp",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|                                              | 0/8 [00:00<?, ?it/s]\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 114.22it/s]\u001b[A\n",
      "batch:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 26/137 [00:00<00:00, 127.89it/s]\u001b[A\n",
      "batch:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 40/137 [00:00<00:00, 132.87it/s]\u001b[A\n",
      "batch:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 55/137 [00:00<00:00, 136.58it/s]\u001b[A\n",
      "batch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 69/137 [00:00<00:00, 135.51it/s]\u001b[A\n",
      "batch:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 83/137 [00:00<00:00, 136.57it/s]\u001b[A\n",
      "batch:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 97/137 [00:00<00:00, 134.55it/s]\u001b[A\n",
      "batch:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 111/137 [00:00<00:00, 133.69it/s]\u001b[A\n",
      "batch:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 126/137 [00:00<00:00, 135.86it/s]\u001b[A\n",
      "train:  12%|â–| 1/8 [00:01<00:07,  1.12s/it, epoch=1, loss=1.68, train_acc=0.763,\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:  10%|â–ˆâ–ˆâ–ˆâ–                              | 14/137 [00:00<00:00, 134.97it/s]\u001b[A\n",
      "batch:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 28/137 [00:00<00:00, 130.69it/s]\u001b[A\n",
      "batch:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 43/137 [00:00<00:00, 138.22it/s]\u001b[A\n",
      "batch:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 58/137 [00:00<00:00, 138.64it/s]\u001b[A\n",
      "batch:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 73/137 [00:00<00:00, 140.32it/s]\u001b[A\n",
      "batch:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 88/137 [00:00<00:00, 140.96it/s]\u001b[A\n",
      "batch:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 103/137 [00:00<00:00, 143.30it/s]\u001b[A\n",
      "batch:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 118/137 [00:00<00:00, 142.41it/s]\u001b[A\n",
      "batch:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 133/137 [00:00<00:00, 140.85it/s]\u001b[A\n",
      "train:  25%|â–Ž| 2/8 [00:02<00:06,  1.11s/it, epoch=2, loss=0.771, train_acc=0.834\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:  10%|â–ˆâ–ˆâ–ˆâ–                              | 14/137 [00:00<00:00, 133.38it/s]\u001b[A\n",
      "batch:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 28/137 [00:00<00:00, 133.40it/s]\u001b[A\n",
      "batch:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 43/137 [00:00<00:00, 139.74it/s]\u001b[A\n",
      "batch:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 57/137 [00:00<00:00, 134.92it/s]\u001b[A\n",
      "batch:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 72/137 [00:00<00:00, 139.89it/s]\u001b[A\n",
      "batch:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 87/137 [00:00<00:00, 138.02it/s]\u001b[A\n",
      "batch:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 103/137 [00:00<00:00, 142.01it/s]\u001b[A\n",
      "batch:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 118/137 [00:00<00:00, 140.52it/s]\u001b[A\n",
      "batch:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 133/137 [00:00<00:00, 141.47it/s]\u001b[A\n",
      "train:  38%|â–| 3/8 [00:03<00:05,  1.10s/it, epoch=3, loss=0.57, train_acc=0.875,\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:  10%|â–ˆâ–ˆâ–ˆâ–                              | 14/137 [00:00<00:00, 131.56it/s]\u001b[A\n",
      "batch:  21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 29/137 [00:00<00:00, 138.35it/s]\u001b[A\n",
      "batch:  32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 44/137 [00:00<00:00, 140.73it/s]\u001b[A\n",
      "batch:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 59/137 [00:00<00:00, 141.20it/s]\u001b[A\n",
      "batch:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 74/137 [00:00<00:00, 141.69it/s]\u001b[A\n",
      "batch:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 89/137 [00:00<00:00, 140.20it/s]\u001b[A\n",
      "batch:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 104/137 [00:00<00:00, 138.54it/s]\u001b[A\n",
      "batch:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 118/137 [00:00<00:00, 133.21it/s]\u001b[A\n",
      "batch:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/137 [00:00<00:00, 128.82it/s]\u001b[A\n",
      "train:  50%|â–Œ| 4/8 [00:04<00:04,  1.12s/it, epoch=4, loss=0.458, train_acc=0.899\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 118.16it/s]\u001b[A\n",
      "batch:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 26/137 [00:00<00:00, 124.83it/s]\u001b[A\n",
      "batch:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 39/137 [00:00<00:00, 121.83it/s]\u001b[A\n",
      "batch:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 52/137 [00:00<00:00, 123.57it/s]\u001b[A\n",
      "batch:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 66/137 [00:00<00:00, 126.76it/s]\u001b[A\n",
      "batch:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 79/137 [00:00<00:00, 125.89it/s]\u001b[A\n",
      "batch:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 92/137 [00:00<00:00, 121.51it/s]\u001b[A\n",
      "batch:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž       | 105/137 [00:00<00:00, 120.32it/s]\u001b[A\n",
      "batch:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 118/137 [00:00<00:00, 122.56it/s]\u001b[A\n",
      "batch:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 131/137 [00:01<00:00, 122.69it/s]\u001b[A\n",
      "train:  62%|â–‹| 5/8 [00:05<00:03,  1.16s/it, epoch=5, loss=0.383, train_acc=0.914\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–ˆâ–                              | 13/137 [00:00<00:00, 125.02it/s]\u001b[A\n",
      "batch:  20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 27/137 [00:00<00:00, 126.76it/s]\u001b[A\n",
      "batch:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 40/137 [00:00<00:00, 123.06it/s]\u001b[A\n",
      "batch:  39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 53/137 [00:00<00:00, 123.79it/s]\u001b[A\n",
      "batch:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 66/137 [00:00<00:00, 123.84it/s]\u001b[A\n",
      "batch:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 79/137 [00:00<00:00, 122.09it/s]\u001b[A\n",
      "batch:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 93/137 [00:00<00:00, 125.20it/s]\u001b[A\n",
      "batch:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 106/137 [00:00<00:00, 124.12it/s]\u001b[A\n",
      "batch:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 119/137 [00:00<00:00, 125.65it/s]\u001b[A\n",
      "batch:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/137 [00:01<00:00, 126.91it/s]\u001b[A\n",
      "train:  75%|â–Š| 6/8 [00:06<00:02,  1.18s/it, epoch=6, loss=0.328, train_acc=0.929\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 119.33it/s]\u001b[A\n",
      "batch:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 25/137 [00:00<00:00, 120.19it/s]\u001b[A\n",
      "batch:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 38/137 [00:00<00:00, 122.73it/s]\u001b[A\n",
      "batch:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 51/137 [00:00<00:00, 120.17it/s]\u001b[A\n",
      "batch:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                  | 64/137 [00:00<00:00, 123.11it/s]\u001b[A\n",
      "batch:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 78/137 [00:00<00:00, 127.81it/s]\u001b[A\n",
      "batch:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 91/137 [00:00<00:00, 128.18it/s]\u001b[A\n",
      "batch:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 104/137 [00:00<00:00, 127.09it/s]\u001b[A\n",
      "batch:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 117/137 [00:00<00:00, 125.11it/s]\u001b[A\n",
      "batch:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 130/137 [00:01<00:00, 126.46it/s]\u001b[A\n",
      "train:  88%|â–‰| 7/8 [00:08<00:01,  1.19s/it, epoch=7, loss=0.285, train_acc=0.937\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–ˆâ–                              | 13/137 [00:00<00:01, 122.54it/s]\u001b[A\n",
      "batch:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 26/137 [00:00<00:00, 120.68it/s]\u001b[A\n",
      "batch:  29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 40/137 [00:00<00:00, 125.39it/s]\u001b[A\n",
      "batch:  40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 55/137 [00:00<00:00, 133.80it/s]\u001b[A\n",
      "batch:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 69/137 [00:00<00:00, 131.15it/s]\u001b[A\n",
      "batch:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 83/137 [00:00<00:00, 128.48it/s]\u001b[A\n",
      "batch:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 96/137 [00:00<00:00, 126.56it/s]\u001b[A\n",
      "batch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 109/137 [00:00<00:00, 119.77it/s]\u001b[A\n",
      "batch:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/137 [00:00<00:00, 119.01it/s]\u001b[A\n",
      "batch:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 135/137 [00:01<00:00, 121.58it/s]\u001b[A\n",
      "train: 100%|â–ˆ| 8/8 [00:09<00:00,  1.17s/it, epoch=8, loss=0.252, train_acc=0.942\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9196\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the logistic regression classifier and run it\n",
    "model = LogisticRegression(TEXT, LABEL).to(device) \n",
    "model.train_all(train_iter, val_iter)\n",
    "model.load_state_dict(model.best_model)\n",
    "test_accuracy = model.evaluate(test_iter)\n",
    "print (f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a295777",
   "metadata": {
    "colab_type": "text",
    "id": "Te35cWGOJlf9"
   },
   "source": [
    "# To Do: Implement a multilayer perceptron\n",
    "\n",
    "## Review of multilayer perceptrons\n",
    "\n",
    "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/multilayer-perceptron-figure.png\" alt=\"multilayer perceptron illustration\" width=\"400\"  align=right />\n",
    "\n",
    "In the last part, you implemented a perceptron, a model that involved a linear calculation (the sum of weights) followed by a nonlinear calculation (the softmax, which converts the summed weight values to probabilities). In a multi-layer perceptron, we take the output of the first perceptron to be the input of a second perceptron (and of course, we could continue on with a third or even more).\n",
    "\n",
    "In this part, you'll implement the forward calculation of a two-layer perceptron, again letting PyTorch handle the backward calculation as well as the optimization of parameters. The first layer will involve a linear summation as before and a **sigmoid** as the nonlinear function. The second will involve a linear summation and a softmax (the latter absorbed, as before, into the loss function). Thus, the difference from the logistic regression implementation is simply the adding of the sigmoid and second linear calculations. See the figure for the structure of the computation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef8438",
   "metadata": {
    "colab_type": "text",
    "id": "FsBnCFe0CnUv"
   },
   "source": [
    "## Implement a multilayer perceptron classifier\n",
    "\n",
    "For the implementation, we ask you to implement a two layer perceptron classifier, again as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You might reuse quite a lot of the code from logistic regression. As before, you need to implement the following methods:\n",
    "\n",
    "1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples, and `hidden_size` specifying the size of the hidden layer (e.g., in the above illustration, `hidden_size` is `D`).\n",
    "\n",
    "    During initialization, you'll want to define two tensors of weights, which serve as the parameters of this model, one for each layer. You'll want to [initialize them randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). \n",
    "    \n",
    "    The weights in the first layer are a kind of lookup (as in the previous part), mapping words to a vector of size `hidden_size`. The [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) is a good way to set up and make use of this weight tensor.\n",
    "    \n",
    "    The weights in the second layer define a linear mapping from vectors of size `hidden_size` to vectors of size `num_labels`. The [`nn.Linear` module](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) or [`torch.mm`](https://pytorch.org/docs/master/generated/torch.mm.html) for matrix multiplication may be helpful here.\n",
    "\n",
    "2. `forward`: Given a text batch of size `batch_size X max_length`, the `forward` function returns a tensor of logits of size `batch_size X num_labels`. \n",
    "\n",
    "    That is, for each text $\\vect{x}$ in the batch and each label $c$, you'll be calculating $MLP(bow(\\vect{x}))$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you don't need to worry about that.\n",
    "    \n",
    "    For the sigmoid sublayer, you might find [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) useful.\n",
    "    \n",
    "3. `train_all`: A method that performs training. You might find lab 1-5 useful.\n",
    "\n",
    "4. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n",
    "\n",
    "You should expect to achieve at least **90%** accuracy on the ATIS classificiation task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "607ced60",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I5TVcZ879gI8"
   },
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "  def __init__ (self, text, label, hidden_size=128): \n",
    "    super().__init__ ()\n",
    "    self.text = text\n",
    "    self.label = label\n",
    "    self.padding_id = text.vocab.stoi[text.pad_token]\n",
    "    self.hidden_size = hidden_size\n",
    "    # Keep the vocabulary sizes available\n",
    "    self.N = len(label.vocab.itos) # num_classes\n",
    "    self.V = len(text.vocab.itos)  # vocab_size\n",
    "    # Specify cross-entropy loss for optimization\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    # TODO: Create and initialize neural modules\n",
    "    self.sigma = nn.Sigmoid()\n",
    "    \n",
    "    # Initialize parameters from randomm uniform sample\n",
    "    self.U_mat = torch.nn.Parameter(0.2 * torch.rand((hidden_size, self.V))-0.1)\n",
    "    self.V_mat = torch.nn.Parameter(torch.rand((self.N, hidden_size)))\n",
    "\n",
    "  def forward(self, text_batch):\n",
    "    # TODO: Calculate the logits for the `text_batch`, \n",
    "    #       returning a tensor of size batch_size x num_labels\n",
    "    X_bow = bag_of_words(text_batch, len(text_batch), self.V)\n",
    "    z = self.sigma(torch.matmul(self.U_mat, X_bow.T))\n",
    "    return torch.matmul(self.V_mat, z).T\n",
    "  \n",
    "  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n",
    "    # Switch the module to training mode\n",
    "    self.train()\n",
    "    # Use Adam to optimize the parameters\n",
    "    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    best_validation_accuracy = -float('inf')\n",
    "    best_model = None\n",
    "    # Run the optimization for multiple epochs\n",
    "    with tqdm(range(epochs), desc='train', position=0) as pbar:\n",
    "      for epoch in pbar:\n",
    "        c_num = 0\n",
    "        total = 0\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_iter, desc='batch', leave=False):\n",
    "          # TODO: set labels, compute logits (Ux in this model), \n",
    "          #       loss, and update parameters\n",
    "          ...\n",
    "          optim.zero_grad()\n",
    "          labels = batch.label\n",
    "          logits = self.forward(batch.text)\n",
    "          loss = self.criterion(logits, labels)\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "          ...\n",
    "          # Prepare to compute the accuracy\n",
    "          predictions = torch.argmax(logits, dim=1)\n",
    "          total += predictions.size(0)\n",
    "          c_num += (predictions == labels).float().sum().item()        \n",
    "          running_loss += loss.item() * predictions.size(0)\n",
    "\n",
    "        # Evaluate and track improvements on the validation dataset\n",
    "        validation_accuracy = self.evaluate(val_iter)\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "          best_validation_accuracy = validation_accuracy\n",
    "          self.best_model = copy.deepcopy(self.state_dict())\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = c_num / total\n",
    "        pbar.set_postfix(epoch=epoch+1, loss=epoch_loss, train_acc = epoch_acc, val_acc=validation_accuracy)\n",
    "\n",
    "  def evaluate(self, iterator):\n",
    "    \"\"\"Returns the model's accuracy on a given dataset `iterator`.\"\"\"\n",
    "    # TODO: Compute accuracy\n",
    "    c_num = 0\n",
    "    total = 0\n",
    "    for batch in iterator:\n",
    "        logits = self.forward(batch.text)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        total += predictions.size(0)\n",
    "        c_num += (predictions == batch.label).float().sum().item()\n",
    "    return c_num / total\n",
    "\n",
    "  def find_misclassified(self, iterator):\n",
    "    for batch in iterator:\n",
    "        logits = self.forward(batch.text)\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "        for pred, label, text in zip(predictions, batch.label, batch.text):\n",
    "            if pred != label:\n",
    "                # Remove <pad> from sentences\n",
    "                clean_text = text\n",
    "                try:\n",
    "                    clean_text = text[:list(text).index(padding_id)]\n",
    "                except: \n",
    "                    pass\n",
    "                # Convert sentence to string\n",
    "                sentence = ' '.join([self.text.vocab.itos[i] for i in clean_text])\n",
    "                predicted = self.label.vocab.itos[pred]\n",
    "                gold_label = self.label.vocab.itos[label]\n",
    "                print(\"sentence: \", sentence)\n",
    "                print(\"predicted: \", predicted , \", label: \", gold_label, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1d0164a",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F4Bs0f7Hv1z4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train:   0%|                                              | 0/8 [00:00<?, ?it/s]\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   7%|â–ˆâ–ˆâ–Œ                                | 10/137 [00:00<00:01, 99.77it/s]\u001b[A\n",
      "batch:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 22/137 [00:00<00:01, 107.52it/s]\u001b[A\n",
      "batch:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 34/137 [00:00<00:00, 111.47it/s]\u001b[A\n",
      "batch:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 46/137 [00:00<00:00, 114.39it/s]\u001b[A\n",
      "batch:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 59/137 [00:00<00:00, 118.00it/s]\u001b[A\n",
      "batch:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                | 71/137 [00:00<00:00, 117.76it/s]\u001b[A\n",
      "batch:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 83/137 [00:00<00:00, 117.72it/s]\u001b[A\n",
      "batch:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 95/137 [00:00<00:00, 116.49it/s]\u001b[A\n",
      "batch:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 107/137 [00:00<00:00, 116.10it/s]\u001b[A\n",
      "batch:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 119/137 [00:01<00:00, 114.28it/s]\u001b[A\n",
      "batch:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 132/137 [00:01<00:00, 117.11it/s]\u001b[A\n",
      "train:  12%|â–| 1/8 [00:01<00:09,  1.32s/it, epoch=1, loss=1.04, train_acc=0.759,\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 114.84it/s]\u001b[A\n",
      "batch:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 24/137 [00:00<00:00, 116.07it/s]\u001b[A\n",
      "batch:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 36/137 [00:00<00:00, 113.58it/s]\u001b[A\n",
      "batch:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 48/137 [00:00<00:00, 112.18it/s]\u001b[A\n",
      "batch:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 61/137 [00:00<00:00, 115.79it/s]\u001b[A\n",
      "batch:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 73/137 [00:00<00:00, 114.90it/s]\u001b[A\n",
      "batch:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž            | 86/137 [00:00<00:00, 116.95it/s]\u001b[A\n",
      "batch:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 98/137 [00:00<00:00, 113.65it/s]\u001b[A\n",
      "batch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 110/137 [00:00<00:00, 115.51it/s]\u001b[A\n",
      "batch:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 122/137 [00:01<00:00, 115.71it/s]\u001b[A\n",
      "batch:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134/137 [00:01<00:00, 116.27it/s]\u001b[A\n",
      "train:  25%|â–Ž| 2/8 [00:02<00:07,  1.31s/it, epoch=2, loss=0.409, train_acc=0.897\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 118.18it/s]\u001b[A\n",
      "batch:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 25/137 [00:00<00:00, 120.40it/s]\u001b[A\n",
      "batch:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 38/137 [00:00<00:00, 122.06it/s]\u001b[A\n",
      "batch:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 51/137 [00:00<00:00, 119.52it/s]\u001b[A\n",
      "batch:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 65/137 [00:00<00:00, 124.75it/s]\u001b[A\n",
      "batch:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 78/137 [00:00<00:00, 122.32it/s]\u001b[A\n",
      "batch:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 91/137 [00:00<00:00, 119.98it/s]\u001b[A\n",
      "batch:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 104/137 [00:00<00:00, 118.72it/s]\u001b[A\n",
      "batch:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 116/137 [00:00<00:00, 118.08it/s]\u001b[A\n",
      "batch:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 128/137 [00:01<00:00, 115.82it/s]\u001b[A\n",
      "train:  38%|â–| 3/8 [00:03<00:06,  1.30s/it, epoch=3, loss=0.276, train_acc=0.933\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 113.66it/s]\u001b[A\n",
      "batch:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 24/137 [00:00<00:01, 112.15it/s]\u001b[A\n",
      "batch:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 36/137 [00:00<00:00, 113.34it/s]\u001b[A\n",
      "batch:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 48/137 [00:00<00:00, 113.68it/s]\u001b[A\n",
      "batch:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 60/137 [00:00<00:00, 114.64it/s]\u001b[A\n",
      "batch:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 72/137 [00:00<00:00, 113.11it/s]\u001b[A\n",
      "batch:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 85/137 [00:00<00:00, 116.80it/s]\u001b[A\n",
      "batch:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 97/137 [00:00<00:00, 117.28it/s]\u001b[A\n",
      "batch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž      | 109/137 [00:00<00:00, 115.79it/s]\u001b[A\n",
      "batch:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 121/137 [00:01<00:00, 114.68it/s]\u001b[A\n",
      "batch:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 133/137 [00:01<00:00, 115.45it/s]\u001b[A\n",
      "train:  50%|â–Œ| 4/8 [00:05<00:05,  1.31s/it, epoch=4, loss=0.203, train_acc=0.955\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 116.14it/s]\u001b[A\n",
      "batch:  18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 25/137 [00:00<00:00, 120.19it/s]\u001b[A\n",
      "batch:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 38/137 [00:00<00:00, 121.64it/s]\u001b[A\n",
      "batch:  37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 51/137 [00:00<00:00, 119.71it/s]\u001b[A\n",
      "batch:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 63/137 [00:00<00:00, 115.95it/s]\u001b[A\n",
      "batch:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 75/137 [00:00<00:00, 116.99it/s]\u001b[A\n",
      "batch:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 87/137 [00:00<00:00, 116.29it/s]\u001b[A\n",
      "batch:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž        | 101/137 [00:00<00:00, 120.49it/s]\u001b[A\n",
      "batch:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 114/137 [00:00<00:00, 120.43it/s]\u001b[A\n",
      "batch:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 127/137 [00:01<00:00, 119.01it/s]\u001b[A\n",
      "train:  62%|â–‹| 5/8 [00:06<00:03,  1.30s/it, epoch=5, loss=0.157, train_acc=0.966\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   8%|â–ˆâ–ˆâ–‹                               | 11/137 [00:00<00:01, 106.32it/s]\u001b[A\n",
      "batch:  17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 23/137 [00:00<00:01, 111.33it/s]\u001b[A\n",
      "batch:  26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 35/137 [00:00<00:00, 113.61it/s]\u001b[A\n",
      "batch:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 48/137 [00:00<00:00, 117.22it/s]\u001b[A\n",
      "batch:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 60/137 [00:00<00:00, 118.12it/s]\u001b[A\n",
      "batch:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 72/137 [00:00<00:00, 117.18it/s]\u001b[A\n",
      "batch:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 85/137 [00:00<00:00, 117.99it/s]\u001b[A\n",
      "batch:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 98/137 [00:00<00:00, 120.52it/s]\u001b[A\n",
      "batch:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 111/137 [00:00<00:00, 117.36it/s]\u001b[A\n",
      "batch:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 123/137 [00:01<00:00, 114.69it/s]\u001b[A\n",
      "batch:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 135/137 [00:01<00:00, 115.57it/s]\u001b[A\n",
      "train:  75%|â–Š| 6/8 [00:07<00:02,  1.30s/it, epoch=6, loss=0.129, train_acc=0.971\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   8%|â–ˆâ–ˆâ–‹                               | 11/137 [00:00<00:01, 103.64it/s]\u001b[A\n",
      "batch:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 22/137 [00:00<00:01, 105.04it/s]\u001b[A\n",
      "batch:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 34/137 [00:00<00:00, 111.01it/s]\u001b[A\n",
      "batch:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 46/137 [00:00<00:00, 111.86it/s]\u001b[A\n",
      "batch:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 58/137 [00:00<00:00, 114.67it/s]\u001b[A\n",
      "batch:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                | 70/137 [00:00<00:00, 115.84it/s]\u001b[A\n",
      "batch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž             | 82/137 [00:00<00:00, 113.46it/s]\u001b[A\n",
      "batch:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž          | 94/137 [00:00<00:00, 114.58it/s]\u001b[A\n",
      "batch:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 106/137 [00:00<00:00, 113.42it/s]\u001b[A\n",
      "batch:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 118/137 [00:01<00:00, 113.26it/s]\u001b[A\n",
      "batch:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 130/137 [00:01<00:00, 112.82it/s]\u001b[A\n",
      "train:  88%|â–‰| 7/8 [00:09<00:01,  1.31s/it, epoch=7, loss=0.107, train_acc=0.977\u001b[A\n",
      "batch:   0%|                                            | 0/137 [00:00<?, ?it/s]\u001b[A\n",
      "batch:   9%|â–ˆâ–ˆâ–‰                               | 12/137 [00:00<00:01, 118.07it/s]\u001b[A\n",
      "batch:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 26/137 [00:00<00:00, 127.54it/s]\u001b[A\n",
      "batch:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 39/137 [00:00<00:00, 122.37it/s]\u001b[A\n",
      "batch:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 52/137 [00:00<00:00, 124.73it/s]\u001b[A\n",
      "batch:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 65/137 [00:00<00:00, 124.45it/s]\u001b[A\n",
      "batch:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž              | 78/137 [00:00<00:00, 125.52it/s]\u001b[A\n",
      "batch:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 91/137 [00:00<00:00, 126.39it/s]\u001b[A\n",
      "batch:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 104/137 [00:00<00:00, 125.07it/s]\u001b[A\n",
      "batch:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 117/137 [00:00<00:00, 124.13it/s]\u001b[A\n",
      "batch:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 131/137 [00:01<00:00, 127.35it/s]\u001b[A\n",
      "train: 100%|â–ˆ| 8/8 [00:10<00:00,  1.29s/it, epoch=8, loss=0.0923, train_acc=0.97\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9397\n"
     ]
    }
   ],
   "source": [
    "# Instantiate classifier and run it\n",
    "model = MultiLayerPerceptron(TEXT, LABEL).to(device) \n",
    "model.train_all(train_iter, val_iter)\n",
    "model.load_state_dict(model.best_model)\n",
    "test_accuracy = model.evaluate(test_iter)\n",
    "print (f'Test accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aed033",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Lessons learned\n",
    "\n",
    "Take a look at some of the examples that were classified correctly and incorrectly by your best method.\n",
    "\n",
    "**Question:** Do you notice anything about the incorrectly classified examples that might indicate _why_ they were classified incorrectly?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_lessons\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0510630f",
   "metadata": {},
   "source": [
    "If you look below, I printed the sentences from the testing data set that were incorrectly classified by the multilayer perceptron, which had the highest accuracy. You can se how some of these questions are even ambiguous for a human to classify and have \\<unk> tokens. For example, the question \"What is \\<unk>\" really could be anything. Also, some sentences request general information so they are hard to classify. For example, \"i need a ticket from nashville tennessee to seattle\" could refer to the price or the flight_id."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee271700",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "105ec426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:  what type of aircraft are flying from cleveland to dallas before noon\n",
      "predicted:  aircraft_code , label:  basic_type \n",
      "\n",
      "sentence:  list <unk> for first class round trip from detroit to st . petersburg\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  give me the flights and fares for a trip to cleveland from miami on wednesday\n",
      "predicted:  fare_id , label:  flight_id \n",
      "\n",
      "sentence:  please find a flight round trip from los angeles to tacoma washington with a stopover in san francisco <unk> <unk> the price of <unk> dollars for june tenth <unk>\n",
      "predicted:  fare_id , label:  flight_id \n",
      "\n",
      "sentence:  list type of aircraft for all flights from charlotte\n",
      "predicted:  aircraft_code , label:  basic_type \n",
      "\n",
      "sentence:  what is a <unk>\n",
      "predicted:  airport_code , label:  aircraft_code \n",
      "\n",
      "sentence:  i ' d like a one way ticket from milwaukee to orlando either wednesday evening or thursday morning\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  get saturday fares from washington to montreal\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  what cities does northwest fly out of\n",
      "predicted:  flight_id , label:  city_code \n",
      "\n",
      "sentence:  what does fare code f mean\n",
      "predicted:  fare_basis_code , label:  booking_class \n",
      "\n",
      "sentence:  what does fare code f mean\n",
      "predicted:  fare_basis_code , label:  booking_class \n",
      "\n",
      "sentence:  list la\n",
      "predicted:  flight_id , label:  city_code \n",
      "\n",
      "sentence:  list la\n",
      "predicted:  flight_id , label:  city_code \n",
      "\n",
      "sentence:  what cities does northwest fly to\n",
      "predicted:  flight_id , label:  city_code \n",
      "\n",
      "sentence:  i need the fares on flights from washington to toronto on a saturday\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  list the cities from which northwest flies\n",
      "predicted:  flight_id , label:  city_code \n",
      "\n",
      "sentence:  what is <unk>\n",
      "predicted:  airport_code , label:  aircraft_code \n",
      "\n",
      "sentence:  list the airlines with flights to or from denver\n",
      "predicted:  flight_id , label:  airline_code \n",
      "\n",
      "sentence:  list distance from airports to downtown in new york\n",
      "predicted:  airport_code , label:  miles_distant \n",
      "\n",
      "sentence:  i need a ticket from nashville to seattle\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  i need flight numbers and airlines for flights departing from oakland to salt lake city on thursday departing before 8am\n",
      "predicted:  flight_id , label:  flight_number \n",
      "\n",
      "sentence:  how many northwest flights leave st . paul\n",
      "predicted:  flight_id , label:  count \n",
      "\n",
      "sentence:  i need a ticket from nashville tennessee to seattle\n",
      "predicted:  flight_id , label:  fare_id \n",
      "\n",
      "sentence:  list flights from orlando to tacoma on saturday of fare <unk> code of q\n",
      "predicted:  booking_class , label:  flight_id \n",
      "\n",
      "sentence:  what is a <unk>\n",
      "predicted:  airport_code , label:  aircraft_code \n",
      "\n",
      "sentence:  list seating <unk> of delta flights from seattle to salt lake city\n",
      "predicted:  flight_id , label:  aircraft_code \n",
      "\n",
      "sentence:  is there ground transportation from the memphis airport into <unk> when if i arrive at <unk> in the morning\n",
      "predicted:  transport_type , label:  state_code \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.find_misclassified(test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc35e02",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this project segment so that we can improve it for later years, and to inform later segments for this year. Please list any issues that arose or comments you have to improve the project segment. Useful things to comment on include the following: \n",
    "\n",
    "* Was the project segment clear or unclear? Which portions?\n",
    "* Were the readings appropriate background for the project segment? \n",
    "* Are there additions or changes you think would make the project segment better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3924649c",
   "metadata": {},
   "source": [
    "The project was very clear in all portions. Maybe an improvement would be writing about how to initialize parameters and how they can change the accuracy. The readings were appropriate and interesting. I would only add some sort of visualization segment to the project. For example, some function that prints the classification of different sentences by the different classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a944acf",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Instructions for submission of the project segment\n",
    "\n",
    "This project segment should be submitted to Gradescope at <http://go.cs187.info/project1-submit>, which will be made available some time before the due date.\n",
    "\n",
    "Project segment notebooks are manually graded, not autograded using otter as labs are. (Otter is used within project segment notebooks to synchronize distribution and solution code however.) **We will not run your notebook before grading it.** Instead, we ask that you submit the already freshly run notebook. The best method is to \"restart kernel and run all cells\", allowing time for all cells to be run to completion.\n",
    "\n",
    "We also request that you **submit a PDF of the freshly run notebook**. The simplest method is to use \"Export notebook to PDF\", which will render the notebook to PDF via LaTeX. If that doesn't work, the method that seems to be most reliable is to export the notebook as HTML (if you are using Jupyter Notebook, you can do so using `File -> Print Preview`), open the HTML in a browser, and print it to a file. Then make sure to add the file to your git commit. Please name the file the same name as this notebook, but with a `.pdf` extension. (Conveniently, the methods just described will use that name by default.) You can then perform a git commit and push and submit the commit to Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49746e75",
   "metadata": {},
   "source": [
    "# End of project segment 1 {-}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "project1_classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "CS187 Project Segment 1: Text Classification"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
